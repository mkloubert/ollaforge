// OllaForge - A web application that simplifies training LLMs with your own data for use in Ollama.
// Copyright (C) 2026  Marcel Joachim Kloubert (marcel@kloubert.dev)
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, either version 3 of the
// License, or (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

import type { TranslationSchema } from "../types";

const nl: TranslationSchema = {
  translation: {
    common: {
      loading: "Laden...",
      error: "Er is een fout opgetreden",
      retry: "Opnieuw proberen",
      cancel: "Annuleren",
      save: "Opslaan",
      delete: "Verwijderen",
      create: "Aanmaken",
      back: "Terug",
      name: "Naam",
      actions: "Acties",
      optional: "optioneel",
      edit: "Bewerken",
      ok: "OK",
      dismiss: "Sluiten",
    },
    app: {
      title: "OllaForge",
    },
    nav: {
      home: "Home",
      projects: "Projecten",
    },
    theme: {
      light: "Licht",
      dark: "Donker",
      system: "Systeem",
      toggle: "Thema wijzigen",
    },
    language: {
      en: "Engels",
      de: "Duits",
      es: "Spaans",
      fr: "Frans",
      pt: "Portugees",
      uk: "Oekraïens",
      zh: "Chinees",
      ja: "Japans",
      ko: "Koreaans",
      ar: "Arabisch",
      hi: "Hindi",
      it: "Italiaans",
      nl: "Nederlands",
      pl: "Pools",
      select: "Selecteer taal",
    },
    api: {
      status: "API Status",
      connected: "Verbonden",
      disconnected: "Niet verbonden",
      checking: "Controleren...",
    },
    huggingface: {
      status: {
        loggedIn: "Ingelogd als {{username}}",
        loggedOut: "Niet ingelogd",
        checking: "Controleren...",
      },
      changeToken: "Token wijzigen",
      login: {
        title: "Hugging Face Login",
        description:
          "Voer je Hugging Face access token in om toegang te krijgen tot beveiligde modellen.",
        tokenLabel: "Access Token",
        tokenPlaceholder: "hf_...",
        help: "Je hebt een Hugging Face access token nodig om beveiligde modellen te downloaden.",
        getToken: "Krijg je token hier",
        submit: "Inloggen",
        submitting: "Bezig met inloggen...",
        success: "Succesvol ingelogd!",
      },
      errors: {
        loginFailed: "Inloggen mislukt. Controleer je token.",
        invalidToken: "Ongeldig token formaat. Token moet beginnen met 'hf_'.",
      },
    },
    llmProviders: {
      title: "LLM Providers",
      status: {
        allValid: "Alle providers geconfigureerd",
        someInvalid: "Sommige providers niet geconfigureerd",
        noneConfigured: "Geen providers geconfigureerd",
        checking: "Controleren...",
        valid: "Geldig",
        invalid: "Niet geconfigureerd",
      },
      providers: {
        openai: "OpenAI",
        anthropic: "Anthropic (Claude)",
        mistral: "Mistral",
      },
      login: {
        title: "{{provider}} configureren",
        description: "Voer je {{provider}} API-sleutel in om integratie in te schakelen.",
        tokenLabel: "API-sleutel",
        help: "Je hebt een API-sleutel nodig om {{provider}} functies te gebruiken.",
        getToken: "Krijg je API-sleutel hier",
        submit: "Opslaan",
        submitting: "Opslaan...",
        success: "API-sleutel succesvol opgeslagen!",
      },
      changeToken: "API-sleutel wijzigen",
      errors: {
        loginFailed: "Kon API-sleutel niet opslaan. Controleer je sleutel.",
      },
    },
    generateFromSources: {
      title: "Genereren uit Bronnen",
      button: "Genereren uit Bronnen",
      buttonDisabled: "Geen LLM provider geconfigureerd",
      sources: {
        title: "Databronnen",
        uploadFile: "Bestand uploaden",
        uploadHint: "Sleep en laat los of klik om te uploaden",
        addText: "Tekst toevoegen",
        textPlaceholder: "Plak je tekst hier...",
        empty: "Nog geen bronnen toegevoegd",
        estimatedTokens: "~{{tokens}} tokens",
        totalTokens: "Totaal: ~{{tokens}} tokens",
        sourcesCount: "{{count}} bron(nen)",
        acceptedFormats: "Geaccepteerd: .txt, .md, .html, .json, .csv, .xml",
        tokenLimitWarning: "Token telling overschrijdt contextvenster limiet ({{limit}} tokens)",
      },
      llm: {
        selectModel: "Selecteer model...",
        contextInfo: "Context: {{context}}K tokens",
        generate: "Genereren",
        generating: "Bezig met genereren...",
        progress: "Verwerking chunk {{current}} van {{total}}...",
        targetLanguage: "Uitvoertaal",
        languages: {
          auto: "Zelfde als invoer",
          en: "Engels",
          de: "Duits",
          es: "Spaans",
          fr: "Frans",
          pt: "Portugees",
          uk: "Oekraïens",
          zh: "Chinees (Vereenvoudigd)",
          ja: "Japans",
          ko: "Koreaans",
          ar: "Arabisch",
          hi: "Hindi",
          it: "Italiaans",
          nl: "Nederlands",
          pl: "Pools",
        },
      },
      results: {
        title: "Gegenereerde Data",
        empty: "Genereer data met het linker paneel",
        instruction: "Instructie",
        output: "Uitvoer",
        addRow: "Rij toevoegen",
        deleteRow: "Verwijderen",
        validRows: "{{valid}} van {{total}} rijen geldig",
        generated: "{{count}} trainingsdata rijen gegenereerd",
      },
      save: {
        title: "Opslaan als JSONL",
        filename: "Bestandsnaam",
        save: "Opslaan",
        saving: "Opslaan...",
        success: "Bestand {{filename}} succesvol opgeslagen",
      },
      errors: {
        emptyCell: "Cel mag niet leeg zijn",
        invalidFilename: "Ongeldige bestandsnaam",
        noData: "Geen data om op te slaan",
        invalidRows: "Corrigeer eerst ongeldige rijen",
      },
    },
    projects: {
      title: "Projecten",
      empty: "Nog geen projecten",
      emptyDescription: "Maak je eerste project om te beginnen.",
      createNew: "Nieuw Project",
      createTitle: "Nieuw Project Aanmaken",
      createDescription: "Voer een naam in voor je nieuwe project.",
      namePlaceholder: "Mijn Project",
      nameLabel: "Projectnaam",
      descriptionLabel: "Beschrijving",
      descriptionPlaceholder: "Een korte beschrijving van je project",
      creating: "Aanmaken...",
      saving: "Opslaan...",
      openProject: "Project openen",
      editTitle: "Project Bewerken",
      editDescription: "Werk de projectnaam en beschrijving bij.",
      deleteTitle: "Project Verwijderen",
      deleteDescription:
        "Weet je zeker dat je \"{{name}}\" wilt verwijderen? Deze actie kan niet ongedaan worden gemaakt.",
      deleting: "Verwijderen...",
      openFolder: "Map openen",
    },
    project: {
      title: "Project",
      backToProjects: "Terug naar Projecten",
      selectModel: "Basismodel",
      selectModelPlaceholder: "Selecteer een model...",
      targetName: "Doelmodel Naam",
      targetNamePlaceholder: "Voer een aangepaste naam in...",
      configuration: "Configuratie",
      status: "Status",
      tabs: {
        basic: "Basis",
        advanced: "Geavanceerd",
      },
      advancedPlaceholder: "Geavanceerde instellingen komen binnenkort...",
    },
    advancedConfig: {
      helpPanel: {
        title: "Over Geavanceerde Instellingen",
        description: "Deze instellingen bepalen hoe je model wordt getraind met QLoRA (Quantized Low-Rank Adaptation). QLoRA maakt het mogelijk om grote taalmodellen te fine-tunen op consumentenhardware met 4-bit kwantisatie en low-rank adapters. De standaardwaarden werken goed voor de meeste gevallen - wijzig ze alleen als je hun impact begrijpt.",
        learnMore: "Meer informatie",
        links: {
          huggingface: "Hugging Face Transformers",
          qlora: "QLoRA Paper",
          lora: "LoRA Uitgelegd",
          transformers: "Training Documentatie",
        },
      },
      trainingParams: {
        title: "Training Parameters",
        numEpochs: "Epochs",
        numEpochsHelp: "Aantal trainingspassen. Meer epochs kunnen de kwaliteit verbeteren maar riskeren overfitting. Aanbevolen: 1-5.",
        batchSize: "Batch Grootte",
        batchSizeHelp: "Samples per trainingsstap. Grotere groottes trainen sneller maar hebben meer geheugen nodig. Aanbevolen: 1-8.",
        gradientAccumulation: "Gradiënt Accumulatie",
        gradientAccumulationHelp: "Accumuleer gradiënten over meerdere stappen. Simuleert grotere batch grootte met minder geheugen.",
        learningRate: "Learning Rate",
        learningRateHelp: "Stapgrootte voor gewichtsupdates. Te hoog veroorzaakt instabiliteit, te laag vertraagt training. Aanbevolen: 1e-5 tot 5e-4.",
        warmupRatio: "Warmup Ratio",
        warmupRatioHelp: "Fractie van training om learning rate geleidelijk te verhogen. Helpt vroege training te stabiliseren.",
        maxLength: "Max Token Lengte",
        maxLengthHelp: "Maximale sequentielengte voor training. Langere sequenties hebben meer geheugen nodig.",
        fp16: "FP16 (Halve Precisie)",
        fp16Help: "Gebruik 16-bit floating point voor snellere training. Alleen beschikbaar op CUDA GPUs.",
        optimizer: "Optimizer",
        optimizerHelp: "Algoritme voor het updaten van gewichten. paged_adamw_8bit is geheugenefficiënt voor QLoRA.",
        optimizers: {
          paged_adamw_8bit: "Paged AdamW 8-bit (aanbevolen voor GPU)",
          adamw_torch: "AdamW (aanbevolen voor CPU)",
          adamw_hf: "AdamW (Hugging Face)",
          sgd: "SGD",
        },
        weightDecay: "Weight Decay",
        weightDecayHelp: "L2 regularisatie om overfitting te voorkomen. Hogere waarden voegen meer regularisatie toe. Aanbevolen: 0.01.",
        maxGradNorm: "Max Gradiënt Norm",
        maxGradNormHelp: "Maximale gradiënt norm voor gradiënt clipping. Voorkomt exploderende gradiënten. Aanbevolen: 1.0.",
        lrScheduler: "LR Scheduler",
        lrSchedulerHelp: "Learning rate scheduler strategie. Bepaalt hoe de learning rate verandert tijdens training.",
        schedulers: {
          linear: "Lineair (aanbevolen)",
          cosine: "Cosinus",
          constant: "Constant",
          polynomial: "Polynoom",
        },
        neftuneNoise: "NEFTune Noise Alpha",
        neftuneNoiseHelp: "Voeg ruis toe aan embeddings tijdens training. Kan generalisatie verbeteren. 0 = uitgeschakeld, 5-15 aanbevolen indien ingeschakeld.",
        seed: "Random Seed",
        seedHelp: "Seed voor reproduceerbaarheid. Gebruik dezelfde seed om identieke resultaten te krijgen over trainingssessies.",
        bf16: "BF16 (Brain Float 16)",
        bf16Help: "Gebruik bfloat16 precisie in plaats van fp16. Alleen beschikbaar op Ampere+ GPUs (RTX 3000+). Betere numerieke stabiliteit dan fp16.",
        loggingSteps: "Logging Stappen",
        loggingStepsHelp: "Log trainingsmetrics elke N stappen. Lagere waarden geven frequentere updates maar kunnen training vertragen.",
        saveStrategy: "Opslag Strategie",
        saveStrategyHelp: "Wanneer model checkpoints opslaan tijdens training.",
        saveStrategies: {
          no: "Geen checkpoints",
          epoch: "Na elke epoch (aanbevolen)",
          steps: "Elke N stappen",
        },
      },
      defaults: {
        showDefaults: "Standaardwaarden gebruiken",
        cudaDefault: "GPU standaard: {{value}}",
        cpuDefault: "CPU standaard: {{value}}",
        resetToDefaults: "Reset naar standaard",
        resetConfirm: "Alle waarden in deze sectie worden gereset naar standaard.",
      },
      loraParams: {
        title: "LoRA Configuratie",
        rank: "Rang (r)",
        rankHelp: "Dimensie van de low-rank matrices. Hogere waarden vangen meer informatie maar gebruiken meer geheugen en riskeren overfitting. Aanbevolen: 8-64.",
        alpha: "Alpha",
        alphaHelp: "Schaalfactor voor LoRA gewichten. Meestal ingesteld op 2x de rang. Hogere waarden vergroten de impact van fine-tuning.",
        dropout: "Dropout",
        dropoutHelp: "Dropout kans voor LoRA lagen. Helpt overfitting voorkomen. Aanbevolen: 0.05-0.1.",
        targetModules: "Doelmodules",
        targetModulesHelp: "Modellagen waarop LoRA wordt toegepast. Meer modules = meer fine-tuning capaciteit maar meer geheugengebruik.",
        modules: {
          q_proj: "Query Projection (q_proj)",
          k_proj: "Key Projection (k_proj)",
          v_proj: "Value Projection (v_proj)",
          o_proj: "Output Projection (o_proj)",
          gate_proj: "Gate Projection (gate_proj)",
          up_proj: "Up Projection (up_proj)",
          down_proj: "Down Projection (down_proj)",
        },
        bias: "Bias Training",
        biasHelp: "Hoe bias termen te behandelen tijdens training. 'none' bevriest biases, 'lora_only' traint LoRA biases, 'all' traint alle biases.",
        biasOptions: {
          none: "Geen (bevries biases)",
          lora_only: "Alleen LoRA",
          all: "Alle biases",
        },
        useRslora: "Gebruik RSLoRA",
        useRsloraHelp: "Rank-Stabilized LoRA verbetert trainingsstabiliteit en prestaties voor hogere rangen (r >= 64). Aanbevolen voor grote rangen.",
        useDora: "Gebruik DoRA (Experimenteel)",
        useDoraHelp: "Weight-Decomposed Low-Rank Adaptation kan fine-tuning kwaliteit verbeteren. Experimentele functie, kan trainingstijd verhogen.",
        modulesToSave: "Modules om op te slaan",
        modulesToSaveHelp: "Extra modules om volledig te trainen (niet met LoRA). Handig voor het trainen van output lagen zoals lm_head.",
        saveModules: {
          lm_head: "Language Model Head (lm_head)",
          embed_tokens: "Token Embeddings (embed_tokens)",
        },
      },
      quantizationParams: {
        title: "Kwantisatie",
        loadIn4bit: "Laden in 4-bit",
        loadIn4bitHelp: "Laad modelgewichten in 4-bit precisie voor verminderd geheugengebruik. Vereist voor QLoRA op beperkt GPU-geheugen. Alleen beschikbaar op CUDA GPUs.",
        quantType: "4-bit Kwantisatie Type",
        quantTypeHelp: "Algoritme voor 4-bit kwantisatie. NF4 (Normal Float 4) wordt aanbevolen voor betere nauwkeurigheid.",
        quantTypes: {
          nf4: "NF4 (aanbevolen)",
          fp4: "FP4",
        },
        doubleQuant: "Dubbele Kwantisatie",
        doubleQuantHelp: "Pas secundaire kwantisatie toe om geheugen verder te verminderen. Kleine nauwkeurigheidsafweging voor aanzienlijke geheugenbesparing.",
        computeDtype: "Compute Dtype",
        computeDtypeHelp: "Datatype voor berekeningen tijdens training. bfloat16 biedt betere numerieke stabiliteit op Ampere+ GPUs (RTX 3000+).",
        computeDtypes: {
          float16: "Float16 (aanbevolen)",
          bfloat16: "BFloat16 (RTX 3000+)",
          float32: "Float32 (volledige precisie)",
        },
        outputQuantization: "Output Kwantisatie",
        outputQuantizationHelp: "Kwantisatieformaat voor het uiteindelijke GGUF model. q8_0 biedt goede balans tussen grootte en kwaliteit.",
        outputTypes: {
          f32: "F32 (volledige precisie, grootste)",
          f16: "F16 (halve precisie)",
          bf16: "BF16 (brain float 16)",
          q8_0: "Q8_0 (8-bit, aanbevolen)",
          auto: "Auto (laat llama.cpp beslissen)",
        },
        cudaOnly: "Deze instellingen zijn alleen van toepassing bij training op een CUDA GPU.",
      },
      modelfileParams: {
        title: "Ollama Modelfile",
        temperature: "Temperatuur",
        temperatureHelp: "Bepaalt willekeurigheid in output. Lagere waarden maken responses gefocusder en deterministischer, hogere waarden creatiever. Aanbevolen: 0.7-0.9.",
        topP: "Top P (Nucleus Sampling)",
        topPHelp: "Overweeg alleen tokens met cumulatieve waarschijnlijkheid tot deze waarde. Lagere waarden focussen op meer waarschijnlijke tokens. Aanbevolen: 0.9.",
        topK: "Top K",
        topKHelp: "Beperk tokenselectie tot de K meest waarschijnlijke opties. Lagere waarden zijn meer gefocust. Aanbevolen: 40.",
        system: "Systeem Prompt",
        systemHelp: "Instructies die bepalen hoe het model zich moet gedragen. Dit stelt de persoonlijkheid en capaciteiten van het model in.",
        systemPlaceholder: "Je bent een behulpzame assistent.",
        stop: "Stop Sequenties",
        stopHelp: "Sequenties die het model signaleren te stoppen met genereren. Meerdere stop sequenties kunnen worden toegevoegd.",
        stopPlaceholder: "Voer een stop sequentie in...",
        stopAdd: "Toevoegen",
        repeatPenalty: "Herhaal Penalty",
        repeatPenaltyHelp: "Penalty voor het herhalen van tokens. Hogere waarden verminderen herhaling. Aanbevolen: 1.1.",
        repeatLastN: "Herhaal Laatste N",
        repeatLastNHelp: "Aantal tokens om te controleren op herhaling. Hogere waarden overwegen meer context. Aanbevolen: 64.",
        numCtx: "Context Venster Grootte",
        numCtxHelp: "Maximale contextlengte voor inferentie. Grotere vensters staan meer context toe maar gebruiken meer geheugen.",
      },
    },
    dataFiles: {
      title: "Trainingsdata",
      empty: "Nog geen databestanden",
      dropzone: "Sleep JSONL bestanden hierheen of klik om te uploaden",
      dropzoneActive: "Laat bestanden hier los...",
      uploadButton: "Bestand uploaden",
      uploading: "Uploaden...",
      deleteConfirm: "Dit bestand verwijderen?",
      previewRowCount: "{{count}} rijen totaal",
      previewTruncated: "toont eerste {{count}}",
      previewEmpty: "Dit bestand bevat geen datarijen.",
      previewError: "Kon bestandsinhoud niet laden.",
      invalidRow: "Ongeldige rij",
      showRawContent: "Toon raw ({{size}})",
      rawContentTitle: "Raw inhoud - Regel {{line}}",
      rawContentLength: "{{count}} tekens",
      errorCount: "{{count}} ongeldige rij(en) gevonden",
      fileStatus: {
        pending: "Wacht op verwerking",
        in_progress: "Laden...",
        completed: "{{loaded}} rijen geladen ({{skipped}} overgeslagen)",
        failed: "Laden mislukt",
        skipped: "Overgeslagen",
      },
      validationErrors: {
        INVALID_JSON: "Ongeldige JSON syntax",
        NOT_OBJECT: "Moet een JSON object zijn",
        MISSING_INSTRUCTION: "Veld \"instruction\" ontbreekt",
        MISSING_OUTPUT: "Veld \"output\" ontbreekt",
        INVALID_INSTRUCTION_TYPE: "\"instruction\" moet een string zijn",
        INVALID_OUTPUT_TYPE: "\"output\" moet een string zijn",
      },
    },
    training: {
      title: "Training",
      startButton: "Model Aanmaken",
      cancelButton: "Training Annuleren",
      noModel: "Selecteer eerst een model",
      noDataFiles: "Voeg eerst minstens één databestand toe",
      readyDescription: "Alles is klaar. Klik op de knop hierboven om je model te maken.",
      status: {
        idle: "Gereed",
        starting: "Starten...",
        loading_data: "Data laden...",
        loading_model: "Model laden...",
        training: "Trainen...",
        exporting: "Model exporteren...",
        converting: "Converteren naar GGUF...",
        completed: "Voltooid",
        failed: "Mislukt",
        cancelled: "Geannuleerd",
      },
      progress: "Voortgang",
      step: "Stap {{current}} van {{total}}",
      device: "Apparaat",
      taskList: "Taken",
      tasks: {
        detect_device: "Compute apparaat detecteren",
        import_libraries: "ML bibliotheken importeren",
        load_model: "Basismodel laden",
        setup_lora: "LoRA adapter instellen",
        tokenize: "Data laden en tokeniseren",
        train: "Model trainen",
        merge_lora: "LoRA samenvoegen met basismodel",
        convert_gguf: "Converteren naar GGUF formaat",
        create_modelfile: "Ollama Modelfile aanmaken",
        register_ollama: "Model registreren in Ollama",
      },
      taskWarnings: "{{count}} ongeldige rij(en) overgeslagen",
      stillWorking: "Nog bezig...",
      errorTitle: "Training Mislukt",
      completed: "Training Voltooid",
      completedDescription: "Je model is succesvol aangemaakt. Controleer de output map voor de Modelfile.",
      cancelled: "Training is geannuleerd.",
    },
    ollama: {
      title: "Ollama Integratie",
      runButton: "Uitvoeren in Ollama",
      modelName: "Modelnaam",
      running: "Terminal openen...",
    },
    presets: {
      title: "Training Presets",
      description: "Quick-start configuraties geoptimaliseerd voor verschillende use cases. Selecteer een preset om de instellingen toe te passen.",
      applyButton: "Toepassen",
      applyConfirmTitle: "Preset Toepassen?",
      applyConfirmDescription: "Dit overschrijft je huidige training, LoRA en kwantisatie instellingen met de \"{{preset}}\" preset waarden. Deze actie kan niet ongedaan worden gemaakt.",
      recommended: "Aanbevolen",
      allModels: "Alle modellen",
      balanced: {
        name: "Gebalanceerd",
        description: "Goede balans tussen snelheid en kwaliteit voor de meeste taken",
        pros: {
          versatile: "Werkt goed met de meeste modellen en data",
          stable: "Stabiele training met bewezen standaardwaarden",
          good_defaults: "Goed startpunt voor experimenteren",
        },
        cons: {
          not_specialized: "Niet geoptimaliseerd voor specifieke use cases",
          moderate_time: "Matige trainingstijd",
        },
      },
      chat: {
        name: "Chat / Conversatie",
        description: "Geoptimaliseerd voor conversationele AI en instructies volgen",
        pros: {
          natural_responses: "Meer natuurlijke conversationele responses",
          instruction_following: "Beter in het volgen van instructies",
          diverse_outputs: "Meer diverse en creatieve outputs",
        },
        cons: {
          more_memory: "Hoger geheugengebruik",
          longer_training: "Langere trainingstijd",
        },
      },
      code: {
        name: "Code Generatie",
        description: "Geoptimaliseerd voor programmeren en code aanvulling",
        pros: {
          precise_syntax: "Precieze syntax learning",
          low_dropout: "Lage dropout voor nauwkeurigheid",
          all_modules: "Richt op alle relevante lagen",
        },
        cons: {
          more_memory: "Hoger geheugengebruik",
          slower_training: "Langzamere trainingssnelheid",
        },
      },
      fast: {
        name: "Snelle Iteratie",
        description: "Snelle training voor snel experimenteren",
        pros: {
          quick_results: "Snelle resultaten voor testen",
          low_memory: "Lagere geheugenvereisten",
          rapid_testing: "Ideaal voor snel prototypen",
        },
        cons: {
          lower_quality: "Lagere output kwaliteit",
          less_learning: "Minder grondig leren",
        },
      },
      high_quality: {
        name: "Hoge Kwaliteit",
        description: "Maximale kwaliteit ten koste van trainingstijd",
        pros: {
          best_results: "Best mogelijke resultaten",
          thorough_learning: "Grondige training over meer epochs",
          all_modules: "Uitgebreide laagdekking",
        },
        cons: {
          long_training: "Lange trainingstijd",
          high_memory: "Hoge geheugenvereisten",
          needs_gpu: "Vereist krachtige GPU",
        },
      },
      low_memory: {
        name: "Laag Geheugen",
        description: "Geminimaliseerd VRAM gebruik voor beperkte hardware",
        pros: {
          minimal_vram: "Minimaal VRAM gebruik",
          works_on_consumer: "Werkt op consumer GPUs",
          gradient_accumulation: "Effectieve gradiënt accumulatie",
        },
        cons: {
          slower_training: "Langzamere trainingssnelheid",
          smaller_rank: "Kleinere LoRA rang beperkt capaciteit",
        },
      },
      multilingual: {
        name: "Meertalig",
        description: "Geoptimaliseerd voor meertalige modellen",
        pros: {
          language_diversity: "Behoudt taaldiversiteit",
          balanced_learning: "Gebalanceerd cross-linguaal leren",
          longer_warmup: "Uitgebreide warmup voor taalaanpassing",
        },
        cons: {
          needs_diverse_data: "Vereist diverse trainingsdata",
          moderate_time: "Matige trainingstijd",
        },
      },
      reasoning: {
        name: "Redeneren / Wiskunde",
        description: "Geoptimaliseerd voor logisch redeneren en wiskunde",
        pros: {
          precise_learning: "Precies en zorgvuldig leren",
          low_dropout: "Lage dropout voor consistentie",
          consistent_outputs: "Meer consistente outputs",
        },
        cons: {
          more_epochs: "Meer training epochs nodig",
          higher_rank: "Hogere rang verhoogt geheugen",
        },
      },
    },
    errors: {
      ERR_PROJECT_1001: "Er bestaat al een project met deze naam.",
      ERR_PROJECT_1002: "Project niet gevonden.",
      ERR_PROJECT_1003: "Ongeldige projectnaam.",
      ERR_PROJECT_1004: "Projectnaam mag niet leeg zijn.",
      ERR_PROJECT_1005: "Project aanmaken mislukt.",
      ERR_PROJECT_1006: "Project verwijderen mislukt.",
      ERR_PROJECT_1007: "Project bijwerken mislukt.",
      ERR_PROJECT_1008: "Projectmap openen mislukt.",
      ERR_MODEL_2001: "Modellenconfiguratie lezen mislukt.",
      ERR_MODEL_2002: "Modellenconfiguratie schrijven mislukt.",
      ERR_DATA_3001: "Databestand niet gevonden.",
      ERR_DATA_3002: "Ongeldig bestandstype. Alleen JSONL bestanden zijn toegestaan.",
      ERR_DATA_3003: "Bestand uploaden mislukt.",
      ERR_DATA_3004: "Bestand verwijderen mislukt.",
      ERR_DATA_3005: "Databestanden lezen mislukt.",
      ERR_TRAINING_4001: "Er draait al een trainingsjob.",
      ERR_TRAINING_4002: "Er draait geen trainingsjob.",
      ERR_TRAINING_4003: "Voeg databestanden toe voordat je de training start.",
      ERR_TRAINING_4004: "Een trainingsdata bestand is niet gevonden.",
      ERR_TRAINING_4005: "Model laden mislukt.",
      ERR_TRAINING_4006: "Training mislukt.",
      ERR_TRAINING_4007: "Model exporteren mislukt.",
      ERR_TRAINING_4008: "Training is geannuleerd.",
      ERR_TRAINING_4009: "llama.cpp niet gevonden. Installeer het eerst.",
      ERR_HF_5001: "Niet ingelogd bij Hugging Face.",
      ERR_HF_5002: "Hugging Face login mislukt.",
      ERR_HF_5003: "Ongeldige Hugging Face token.",
      ERR_OLLAMA_6001: "Ollama is niet geïnstalleerd.",
      ERR_OLLAMA_6002: "Ollama draait niet.",
      ERR_OLLAMA_6003: "Model aanmaken in Ollama mislukt.",
      ERR_OLLAMA_6004: "Model niet gevonden in Ollama.",
      ERR_OLLAMA_6005: "Modelfile niet gevonden. Train eerst het model.",
      ERR_OLLAMA_6006: "Terminal openen mislukt.",
      ERR_OLLAMA_6007: "Geen basismodel geconfigureerd. Selecteer eerst een model.",
      ERR_LLM_8001: "Ongeldig API-sleutel formaat.",
      ERR_LLM_8002: "API-sleutel geweigerd. Controleer je sleutel.",
      ERR_LLM_8003: "Onbekende provider.",
      ERR_LLM_8004: "Kon provider API niet bereiken.",
      ERR_LLM_8005: "Kon API-sleutel niet opslaan.",
      ERR_DATA_SOURCE_9001: "Ongeldig bestandstype voor databron.",
      ERR_DATA_SOURCE_9002: "Bestand is te groot.",
      ERR_DATA_SOURCE_9003: "Databron is leeg.",
      ERR_GENERATION_9101: "LLM provider is niet geconfigureerd.",
      ERR_GENERATION_9102: "Model is niet beschikbaar.",
      ERR_GENERATION_9103: "Token limiet overschreden. Gebruik kleinere databronnen.",
      ERR_GENERATION_9104: "Fout bij communicatie met LLM API.",
      ERR_GENERATION_9105: "Ongeldige response van LLM.",
      ERR_GENERATION_9106: "Rate limit overschreden. Wacht even en probeer opnieuw.",
      ERR_SAVE_9201: "Ongeldige bestandsnaam.",
      ERR_SAVE_9202: "Bestand opslaan mislukt.",
      unknown: "Er is een onverwachte fout opgetreden.",
    },
    validation: {
      mustBeInteger: "Moet een geheel getal zijn",
      mustBeNumber: "Moet een geldig getal zijn",
      mustBeString: "Moet tekst zijn",
      mustBeBoolean: "Moet waar of onwaar zijn",
      mustBeArray: "Moet een lijst zijn",
      mustBeGreaterThan: "Moet groter zijn dan {{min}}",
      mustBeAtLeast: "Moet minstens {{min}} zijn",
      mustBeAtMost: "Moet hoogstens {{max}} zijn",
      maxLength: "Maximaal {{maxLength}} tekens",
    },
  },
};

export default nl;
