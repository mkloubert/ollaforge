// OllaForge - A web application that simplifies training LLMs with your own data for use in Ollama.
// Copyright (C) 2026  Marcel Joachim Kloubert (marcel@kloubert.dev)
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, either version 3 of the
// License, or (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

import type { TranslationSchema } from "../types";

const en: TranslationSchema = {
  translation: {
    common: {
      loading: "Loading...",
      error: "An error occurred",
      retry: "Retry",
      cancel: "Cancel",
      save: "Save",
      delete: "Delete",
      create: "Create",
      back: "Back",
      name: "Name",
      actions: "Actions",
      optional: "optional",
      edit: "Edit",
      ok: "OK",
      dismiss: "Dismiss",
    },
    app: {
      title: "OllaForge",
    },
    nav: {
      home: "Home",
      projects: "Projects",
    },
    theme: {
      light: "Light",
      dark: "Dark",
      system: "System",
      toggle: "Toggle theme",
    },
    language: {
      en: "English",
      de: "German",
      select: "Select language",
    },
    api: {
      status: "API Status",
      connected: "Connected",
      disconnected: "Disconnected",
      checking: "Checking...",
    },
    huggingface: {
      status: {
        loggedIn: "Logged in as {{username}}",
        loggedOut: "Not logged in",
        checking: "Checking...",
      },
      changeToken: "Change token",
      login: {
        title: "Hugging Face Login",
        description:
          "Enter your Hugging Face access token to access gated models.",
        tokenLabel: "Access Token",
        tokenPlaceholder: "hf_...",
        help: "You need a Hugging Face access token to download gated models.",
        getToken: "Get your token here",
        submit: "Login",
        submitting: "Logging in...",
        success: "Successfully logged in!",
      },
      errors: {
        loginFailed: "Login failed. Please check your token.",
        invalidToken: "Invalid token format. Token must start with 'hf_'.",
      },
    },
    projects: {
      title: "Projects",
      empty: "No projects yet",
      emptyDescription: "Create your first project to get started.",
      createNew: "New Project",
      createTitle: "Create New Project",
      createDescription: "Enter a name for your new project.",
      namePlaceholder: "My Project",
      nameLabel: "Project Name",
      descriptionLabel: "Description",
      descriptionPlaceholder: "A brief description of your project",
      creating: "Creating...",
      saving: "Saving...",
      openProject: "Open project",
      editTitle: "Edit Project",
      editDescription: "Update the project name and description.",
      deleteTitle: "Delete Project",
      deleteDescription:
        "Are you sure you want to delete \"{{name}}\"? This action cannot be undone.",
      deleting: "Deleting...",
    },
    project: {
      title: "Project",
      backToProjects: "Back to Projects",
      selectModel: "Base Model",
      selectModelPlaceholder: "Select a model...",
      targetName: "Target Model Name",
      targetNamePlaceholder: "Enter a custom name...",
      configuration: "Configuration",
      status: "Status",
      tabs: {
        basic: "Basic",
        advanced: "Advanced",
      },
      advancedPlaceholder: "Advanced settings coming soon...",
    },
    advancedConfig: {
      helpPanel: {
        title: "About Advanced Settings",
        description: "These settings control how your model is trained using QLoRA (Quantized Low-Rank Adaptation). QLoRA enables fine-tuning large language models on consumer hardware by using 4-bit quantization and low-rank adapters. The default values work well for most use cases - only change them if you understand their impact.",
        learnMore: "Learn more",
        links: {
          huggingface: "Hugging Face Transformers",
          qlora: "QLoRA Paper",
          lora: "LoRA Explained",
          transformers: "Training Documentation",
        },
      },
      trainingParams: {
        title: "Training Parameters",
        numEpochs: "Epochs",
        numEpochsHelp: "Number of training passes. More epochs can improve quality but risk overfitting. Recommended: 1-5.",
        batchSize: "Batch Size",
        batchSizeHelp: "Samples per training step. Larger sizes train faster but need more memory. Recommended: 1-8.",
        gradientAccumulation: "Gradient Accumulation",
        gradientAccumulationHelp: "Accumulate gradients over multiple steps. Simulates larger batch size with less memory.",
        learningRate: "Learning Rate",
        learningRateHelp: "Step size for weight updates. Too high causes instability, too low slows training. Recommended: 1e-5 to 5e-4.",
        warmupRatio: "Warmup Ratio",
        warmupRatioHelp: "Fraction of training to gradually increase learning rate. Helps stabilize early training.",
        maxLength: "Max Token Length",
        maxLengthHelp: "Maximum sequence length for training. Longer sequences need more memory.",
        fp16: "FP16 (Half Precision)",
        fp16Help: "Use 16-bit floating point for faster training. Only available on CUDA GPUs.",
        optimizer: "Optimizer",
        optimizerHelp: "Algorithm for updating weights. paged_adamw_8bit is memory efficient for QLoRA.",
        optimizers: {
          paged_adamw_8bit: "Paged AdamW 8-bit (recommended for GPU)",
          adamw_torch: "AdamW (recommended for CPU)",
          adamw_hf: "AdamW (Hugging Face)",
          sgd: "SGD",
        },
      },
      defaults: {
        showDefaults: "Using defaults",
        cudaDefault: "GPU default: {{value}}",
        cpuDefault: "CPU default: {{value}}",
        resetToDefaults: "Reset to defaults",
        resetConfirm: "All values in this section will be reset to their defaults.",
      },
      loraParams: {
        title: "LoRA Configuration",
        rank: "Rank (r)",
        rankHelp: "Dimension of the low-rank matrices. Higher values capture more information but use more memory and risk overfitting. Recommended: 8-64.",
        alpha: "Alpha",
        alphaHelp: "Scaling factor for LoRA weights. Typically set to 2x the rank. Higher values increase the impact of fine-tuning.",
        dropout: "Dropout",
        dropoutHelp: "Dropout probability for LoRA layers. Helps prevent overfitting. Recommended: 0.05-0.1.",
        targetModules: "Target Modules",
        targetModulesHelp: "Model layers to apply LoRA to. More modules = more fine-tuning capacity but more memory usage.",
        modules: {
          q_proj: "Query Projection (q_proj)",
          k_proj: "Key Projection (k_proj)",
          v_proj: "Value Projection (v_proj)",
          o_proj: "Output Projection (o_proj)",
          gate_proj: "Gate Projection (gate_proj)",
          up_proj: "Up Projection (up_proj)",
          down_proj: "Down Projection (down_proj)",
        },
      },
      quantizationParams: {
        title: "Quantization",
        loadIn4bit: "Load in 4-bit",
        loadIn4bitHelp: "Load model weights in 4-bit precision for reduced memory usage. Required for QLoRA on limited GPU memory. Only available on CUDA GPUs.",
        quantType: "4-bit Quantization Type",
        quantTypeHelp: "Algorithm for 4-bit quantization. NF4 (Normal Float 4) is recommended for better accuracy.",
        quantTypes: {
          nf4: "NF4 (recommended)",
          fp4: "FP4",
        },
        doubleQuant: "Double Quantization",
        doubleQuantHelp: "Apply secondary quantization to reduce memory further. Small accuracy trade-off for significant memory savings.",
        outputQuantization: "Output Quantization",
        outputQuantizationHelp: "Quantization format for the final GGUF model. q8_0 offers good balance between size and quality.",
        outputTypes: {
          f32: "F32 (full precision, largest)",
          f16: "F16 (half precision)",
          bf16: "BF16 (brain float 16)",
          q8_0: "Q8_0 (8-bit, recommended)",
          auto: "Auto (let llama.cpp decide)",
        },
        cudaOnly: "These settings only apply when training on a CUDA GPU.",
      },
    },
    dataFiles: {
      title: "Training Data",
      empty: "No data files yet",
      dropzone: "Drop JSONL files here or click to upload",
      dropzoneActive: "Drop files here...",
      uploadButton: "Upload File",
      uploading: "Uploading...",
      deleteConfirm: "Delete this file?",
      previewRowCount: "{{count}} rows total",
      previewTruncated: "showing first {{count}}",
      previewEmpty: "This file contains no data rows.",
      previewError: "Could not load file content.",
      invalidRow: "Invalid row",
      showRawContent: "Show raw ({{size}})",
      rawContentTitle: "Raw content - Line {{line}}",
      rawContentLength: "{{count}} characters",
      errorCount: "{{count}} invalid row(s) found",
      fileStatus: {
        pending: "Waiting to be processed",
        in_progress: "Loading...",
        completed: "Loaded {{loaded}} rows ({{skipped}} skipped)",
        failed: "Failed to load",
        skipped: "Skipped",
      },
      validationErrors: {
        INVALID_JSON: "Invalid JSON syntax",
        NOT_OBJECT: "Must be a JSON object",
        MISSING_INSTRUCTION: "Missing \"instruction\" field",
        MISSING_OUTPUT: "Missing \"output\" field",
        INVALID_INSTRUCTION_TYPE: "\"instruction\" must be a string",
        INVALID_OUTPUT_TYPE: "\"output\" must be a string",
      },
    },
    training: {
      title: "Training",
      startButton: "Create Model",
      cancelButton: "Cancel Training",
      noModel: "Please select a model first",
      noDataFiles: "Please add at least one data file",
      readyDescription: "Everything is set up. Click the button above to start creating your model.",
      status: {
        idle: "Ready",
        starting: "Starting...",
        loading_data: "Loading data...",
        loading_model: "Loading model...",
        training: "Training...",
        exporting: "Exporting model...",
        converting: "Converting to GGUF...",
        completed: "Completed",
        failed: "Failed",
        cancelled: "Cancelled",
      },
      progress: "Progress",
      step: "Step {{current}} of {{total}}",
      device: "Device",
      taskList: "Tasks",
      tasks: {
        detect_device: "Detect compute device",
        import_libraries: "Import ML libraries",
        load_model: "Load base model",
        setup_lora: "Set up LoRA adapter",
        tokenize: "Load and tokenize data",
        train: "Train model",
        merge_lora: "Merge LoRA with base model",
        convert_gguf: "Convert to GGUF format",
        create_modelfile: "Create Ollama Modelfile",
      },
      taskWarnings: "{{count}} invalid row(s) skipped",
      errorTitle: "Training Failed",
      completed: "Training Completed",
      completedDescription: "Your model was created successfully. Check the output folder for the Modelfile.",
      cancelled: "Training was cancelled.",
    },
    ollama: {
      title: "Ollama Integration",
      createButton: "Create in Ollama",
      runButton: "Run in Ollama",
      modelName: "Model name",
      creating: "Creating model...",
      running: "Opening terminal...",
    },
    errors: {
      ERR_PROJECT_1001: "A project with this name already exists.",
      ERR_PROJECT_1002: "Project not found.",
      ERR_PROJECT_1003: "Invalid project name.",
      ERR_PROJECT_1004: "Project name cannot be empty.",
      ERR_PROJECT_1005: "Failed to create project.",
      ERR_PROJECT_1006: "Failed to delete project.",
      ERR_PROJECT_1007: "Failed to update project.",
      ERR_MODEL_2001: "Failed to read models configuration.",
      ERR_MODEL_2002: "Failed to write models configuration.",
      ERR_DATA_3001: "Data file not found.",
      ERR_DATA_3002: "Invalid file type. Only JSONL files are allowed.",
      ERR_DATA_3003: "Failed to upload file.",
      ERR_DATA_3004: "Failed to delete file.",
      ERR_DATA_3005: "Failed to read data files.",
      ERR_TRAINING_4001: "A training job is already running.",
      ERR_TRAINING_4002: "No training job is running.",
      ERR_TRAINING_4003: "Please add data files before starting training.",
      ERR_TRAINING_4004: "A training data file was not found.",
      ERR_TRAINING_4005: "Failed to load the model.",
      ERR_TRAINING_4006: "Training failed.",
      ERR_TRAINING_4007: "Failed to export the model.",
      ERR_TRAINING_4008: "Training was cancelled.",
      ERR_TRAINING_4009: "llama.cpp was not found. Please install it first.",
      ERR_HF_5001: "Not logged in to Hugging Face.",
      ERR_HF_5002: "Hugging Face login failed.",
      ERR_HF_5003: "Invalid Hugging Face token.",
      ERR_OLLAMA_6001: "Ollama is not installed.",
      ERR_OLLAMA_6002: "Ollama is not running.",
      ERR_OLLAMA_6003: "Failed to create model in Ollama.",
      ERR_OLLAMA_6004: "Model not found in Ollama.",
      ERR_OLLAMA_6005: "Modelfile not found. Please train the model first.",
      ERR_OLLAMA_6006: "Failed to open terminal.",
      ERR_OLLAMA_6007: "No base model configured. Please select a model first.",
      unknown: "An unexpected error occurred.",
    },
  },
};

export default en;
