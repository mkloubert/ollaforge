// OllaForge - A web application that simplifies training LLMs with your own data for use in Ollama.
// Copyright (C) 2026  Marcel Joachim Kloubert (marcel@kloubert.dev)
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, either version 3 of the
// License, or (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

import type { TranslationSchema } from "../types";

const zh: TranslationSchema = {
  translation: {
    common: {
      loading: "加载中...",
      error: "发生错误",
      retry: "重试",
      cancel: "取消",
      save: "保存",
      delete: "删除",
      create: "创建",
      back: "返回",
      name: "名称",
      actions: "操作",
      optional: "可选",
      edit: "编辑",
      ok: "确定",
      dismiss: "关闭",
    },
    app: {
      title: "OllaForge",
    },
    nav: {
      home: "首页",
      projects: "项目",
    },
    theme: {
      light: "浅色",
      dark: "深色",
      system: "系统",
      toggle: "切换主题",
    },
    language: {
      en: "英语",
      de: "德语",
      es: "西班牙语",
      fr: "法语",
      pt: "葡萄牙语",
      uk: "乌克兰语",
      zh: "中文",
      ja: "日语",
      ko: "韩语",
      ar: "阿拉伯语",
      hi: "印地语",
      it: "意大利语",
      nl: "荷兰语",
      pl: "波兰语",
      select: "选择语言",
    },
    api: {
      status: "API状态",
      connected: "已连接",
      disconnected: "未连接",
      checking: "检查中...",
    },
    huggingface: {
      status: {
        loggedIn: "已登录为 {{username}}",
        loggedOut: "未登录",
        checking: "检查中...",
      },
      changeToken: "更改令牌",
      login: {
        title: "Hugging Face 登录",
        description: "输入您的 Hugging Face 访问令牌以访问受保护的模型。",
        tokenLabel: "访问令牌",
        tokenPlaceholder: "hf_...",
        help: "您需要 Hugging Face 访问令牌才能下载受保护的模型。",
        getToken: "在此获取令牌",
        submit: "登录",
        submitting: "登录中...",
        success: "登录成功！",
      },
      errors: {
        loginFailed: "登录失败。请检查您的令牌。",
        invalidToken: "令牌格式无效。令牌必须以 'hf_' 开头。",
      },
    },
    llmProviders: {
      title: "LLM 提供商",
      status: {
        allValid: "所有提供商已配置",
        someInvalid: "部分提供商未配置",
        noneConfigured: "没有配置提供商",
        checking: "检查中...",
        valid: "有效",
        invalid: "未配置",
      },
      providers: {
        openai: "OpenAI",
        anthropic: "Anthropic (Claude)",
        mistral: "Mistral",
      },
      login: {
        title: "配置 {{provider}}",
        description: "输入您的 {{provider}} API 密钥以启用集成。",
        tokenLabel: "API 密钥",
        help: "您需要 API 密钥才能使用 {{provider}} 功能。",
        getToken: "在此获取 API 密钥",
        submit: "保存",
        submitting: "保存中...",
        success: "API 密钥保存成功！",
      },
      changeToken: "更改 API 密钥",
      errors: {
        loginFailed: "保存 API 密钥失败。请检查您的密钥。",
      },
    },
    generateFromSources: {
      title: "从数据源生成",
      button: "从数据源生成",
      buttonDisabled: "未配置 LLM 提供商",
      sources: {
        title: "数据源",
        uploadFile: "上传文件",
        uploadHint: "拖放或点击上传",
        addText: "添加文本",
        textPlaceholder: "在此粘贴文本...",
        empty: "尚未添加数据源",
        estimatedTokens: "约 {{tokens}} 个令牌",
        totalTokens: "总计：约 {{tokens}} 个令牌",
        sourcesCount: "{{count}} 个数据源",
        acceptedFormats: "接受格式：.txt, .md, .html, .json, .csv, .xml",
        tokenLimitWarning: "令牌数超过上下文窗口限制（{{limit}} 个令牌）",
      },
      llm: {
        selectModel: "选择模型...",
        contextInfo: "上下文：{{context}}K 令牌",
        generate: "生成",
        generating: "生成中...",
        progress: "处理片段 {{current}}/{{total}}...",
        targetLanguage: "输出语言",
        languages: {
          auto: "与输入相同",
          en: "英语",
          de: "德语",
          es: "西班牙语",
          fr: "法语",
          pt: "葡萄牙语",
          uk: "乌克兰语",
          zh: "中文（简体）",
          ja: "日语",
          ko: "韩语",
          ar: "阿拉伯语",
          hi: "印地语",
          it: "意大利语",
          nl: "荷兰语",
          pl: "波兰语",
        },
      },
      results: {
        title: "生成的数据",
        empty: "使用左侧面板生成数据",
        instruction: "指令",
        output: "输出",
        addRow: "添加行",
        deleteRow: "删除",
        validRows: "{{valid}}/{{total}} 行有效",
        generated: "已生成 {{count}} 行训练数据",
      },
      save: {
        title: "保存为 JSONL",
        filename: "文件名",
        save: "保存",
        saving: "保存中...",
        success: "文件 {{filename}} 保存成功",
      },
      errors: {
        emptyCell: "单元格不能为空",
        invalidFilename: "无效的文件名",
        noData: "没有数据可保存",
        invalidRows: "请先修复无效行",
      },
    },
    projects: {
      title: "项目",
      empty: "暂无项目",
      emptyDescription: "创建您的第一个项目以开始。",
      createNew: "新建项目",
      createTitle: "创建新项目",
      createDescription: "为新项目输入名称。",
      namePlaceholder: "我的项目",
      nameLabel: "项目名称",
      descriptionLabel: "描述",
      descriptionPlaceholder: "项目的简短描述",
      creating: "创建中...",
      saving: "保存中...",
      openProject: "打开项目",
      editTitle: "编辑项目",
      editDescription: "更新项目名称和描述。",
      deleteTitle: "删除项目",
      deleteDescription: "确定要删除 \"{{name}}\" 吗？此操作无法撤销。",
      deleting: "删除中...",
      openFolder: "打开文件夹",
    },
    project: {
      title: "项目",
      backToProjects: "返回项目列表",
      selectModel: "基础模型",
      selectModelPlaceholder: "选择模型...",
      targetName: "目标模型名称",
      targetNamePlaceholder: "输入自定义名称...",
      configuration: "配置",
      status: "状态",
      tabs: {
        basic: "基础",
        advanced: "高级",
      },
      advancedPlaceholder: "高级设置即将推出...",
    },
    advancedConfig: {
      helpPanel: {
        title: "关于高级设置",
        description: "这些设置控制如何使用 QLoRA（量化低秩适应）训练模型。QLoRA 使用 4 位量化和低秩适配器在消费级硬件上微调大型语言模型。默认值适用于大多数用例 - 仅在了解其影响时才更改。",
        learnMore: "了解更多",
        links: {
          huggingface: "Hugging Face Transformers",
          qlora: "QLoRA 论文",
          lora: "LoRA 解释",
          transformers: "训练文档",
        },
      },
      trainingParams: {
        title: "训练参数",
        numEpochs: "轮次",
        numEpochsHelp: "训练遍数。更多轮次可以提高质量，但有过拟合风险。推荐：1-5。",
        batchSize: "批次大小",
        batchSizeHelp: "每步训练的样本数。更大的批次训练更快，但需要更多内存。推荐：1-8。",
        gradientAccumulation: "梯度累积",
        gradientAccumulationHelp: "跨多步累积梯度。用更少的内存模拟更大的批次大小。",
        learningRate: "学习率",
        learningRateHelp: "权重更新的步长。太高会导致不稳定，太低会减慢训练。推荐：1e-5 到 5e-4。",
        warmupRatio: "预热比例",
        warmupRatioHelp: "逐渐增加学习率的训练比例。有助于稳定早期训练。",
        maxLength: "最大令牌长度",
        maxLengthHelp: "训练的最大序列长度。较长的序列需要更多内存。",
        fp16: "FP16（半精度）",
        fp16Help: "使用 16 位浮点进行更快的训练。仅在 CUDA GPU 上可用。",
        optimizer: "优化器",
        optimizerHelp: "更新权重的算法。paged_adamw_8bit 对 QLoRA 内存效率高。",
        optimizers: {
          paged_adamw_8bit: "Paged AdamW 8-bit（推荐用于 GPU）",
          adamw_torch: "AdamW（推荐用于 CPU）",
          adamw_hf: "AdamW（Hugging Face）",
          sgd: "SGD",
        },
        weightDecay: "权重衰减",
        weightDecayHelp: "防止过拟合的 L2 正则化。更高的值增加更多正则化。推荐：0.01。",
        maxGradNorm: "最大梯度范数",
        maxGradNormHelp: "梯度裁剪的最大梯度范数。防止梯度爆炸。推荐：1.0。",
        lrScheduler: "学习率调度器",
        lrSchedulerHelp: "学习率调度策略。控制训练期间学习率如何变化。",
        schedulers: {
          linear: "线性（推荐）",
          cosine: "余弦",
          constant: "常数",
          polynomial: "多项式",
        },
        neftuneNoise: "NEFTune 噪声 Alpha",
        neftuneNoiseHelp: "在训练期间向嵌入添加噪声。可以改善泛化。0 = 禁用，启用时推荐 5-15。",
        seed: "随机种子",
        seedHelp: "可重复性种子。使用相同种子在训练运行之间获得相同结果。",
        bf16: "BF16（Brain Float 16）",
        bf16Help: "使用 bfloat16 精度而不是 fp16。仅在 Ampere+ GPU（RTX 3000+）上可用。比 fp16 具有更好的数值稳定性。",
        loggingSteps: "日志步数",
        loggingStepsHelp: "每 N 步记录训练指标。较低的值提供更频繁的更新，但可能会减慢训练。",
        saveStrategy: "保存策略",
        saveStrategyHelp: "训练期间何时保存模型检查点。",
        saveStrategies: {
          no: "无检查点",
          epoch: "每个轮次后（推荐）",
          steps: "每 N 步",
        },
      },
      defaults: {
        showDefaults: "使用默认值",
        cudaDefault: "GPU 默认：{{value}}",
        cpuDefault: "CPU 默认：{{value}}",
        resetToDefaults: "重置为默认值",
        resetConfirm: "此部分的所有值将重置为默认值。",
      },
      loraParams: {
        title: "LoRA 配置",
        rank: "秩 (r)",
        rankHelp: "低秩矩阵的维度。更高的值捕获更多信息，但使用更多内存并有过拟合风险。推荐：8-64。",
        alpha: "Alpha",
        alphaHelp: "LoRA 权重的缩放因子。通常设置为秩的 2 倍。更高的值增加微调的影响。",
        dropout: "Dropout",
        dropoutHelp: "LoRA 层的 dropout 概率。有助于防止过拟合。推荐：0.05-0.1。",
        targetModules: "目标模块",
        targetModulesHelp: "应用 LoRA 的模型层。更多模块 = 更多微调能力，但更多内存使用。",
        modules: {
          q_proj: "查询投影 (q_proj)",
          k_proj: "键投影 (k_proj)",
          v_proj: "值投影 (v_proj)",
          o_proj: "输出投影 (o_proj)",
          gate_proj: "门控投影 (gate_proj)",
          up_proj: "上投影 (up_proj)",
          down_proj: "下投影 (down_proj)",
        },
        bias: "偏置训练",
        biasHelp: "训练期间如何处理偏置项。'none' 冻结偏置，'lora_only' 训练 LoRA 偏置，'all' 训练所有偏置。",
        biasOptions: {
          none: "无（冻结偏置）",
          lora_only: "仅 LoRA",
          all: "所有偏置",
        },
        useRslora: "使用 RSLoRA",
        useRsloraHelp: "秩稳定 LoRA 提高较高秩（r >= 64）的训练稳定性和性能。推荐用于大秩。",
        useDora: "使用 DoRA（实验性）",
        useDoraHelp: "权重分解低秩适应可以提高微调质量。实验功能，可能会增加训练时间。",
        modulesToSave: "要保存的模块",
        modulesToSaveHelp: "要完全训练的额外模块（不使用 LoRA）。对于训练输出层如 lm_head 很有用。",
        saveModules: {
          lm_head: "语言模型头 (lm_head)",
          embed_tokens: "令牌嵌入 (embed_tokens)",
        },
      },
      quantizationParams: {
        title: "量化",
        loadIn4bit: "4 位加载",
        loadIn4bitHelp: "以 4 位精度加载模型权重以减少内存使用。在有限 GPU 内存上需要 QLoRA。仅在 CUDA GPU 上可用。",
        quantType: "4 位量化类型",
        quantTypeHelp: "4 位量化算法。NF4（Normal Float 4）推荐用于更好的精度。",
        quantTypes: {
          nf4: "NF4（推荐）",
          fp4: "FP4",
        },
        doubleQuant: "双重量化",
        doubleQuantHelp: "应用二次量化以进一步减少内存。以较小的精度换取显著的内存节省。",
        computeDtype: "计算数据类型",
        computeDtypeHelp: "训练期间计算的数据类型。bfloat16 在 Ampere+ GPU（RTX 3000+）上提供更好的数值稳定性。",
        computeDtypes: {
          float16: "Float16（推荐）",
          bfloat16: "BFloat16（RTX 3000+）",
          float32: "Float32（全精度）",
        },
        outputQuantization: "输出量化",
        outputQuantizationHelp: "最终 GGUF 模型的量化格式。q8_0 在大小和质量之间提供良好的平衡。",
        outputTypes: {
          f32: "F32（全精度，最大）",
          f16: "F16（半精度）",
          bf16: "BF16（brain float 16）",
          q8_0: "Q8_0（8 位，推荐）",
          auto: "自动（让 llama.cpp 决定）",
        },
        cudaOnly: "这些设置仅在 CUDA GPU 上训练时适用。",
      },
      modelfileParams: {
        title: "Ollama Modelfile",
        temperature: "温度",
        temperatureHelp: "控制输出的随机性。较低的值使响应更集中和确定性，较高的值更有创意。推荐：0.7-0.9。",
        topP: "Top P（核采样）",
        topPHelp: "仅考虑累积概率达到此值的令牌。较低的值关注更可能的令牌。推荐：0.9。",
        topK: "Top K",
        topKHelp: "将令牌选择限制为 K 个最可能的选项。较低的值更集中。推荐：40。",
        system: "系统提示",
        systemHelp: "定义模型行为的指令。这设置模型的个性和能力。",
        systemPlaceholder: "你是一个有帮助的助手。",
        stop: "停止序列",
        stopHelp: "信号模型停止生成的序列。可以添加多个停止序列。",
        stopPlaceholder: "输入停止序列...",
        stopAdd: "添加",
        repeatPenalty: "重复惩罚",
        repeatPenaltyHelp: "重复令牌的惩罚。较高的值减少重复。推荐：1.1。",
        repeatLastN: "重复检查最后 N",
        repeatLastNHelp: "检查重复的令牌数。较高的值考虑更多上下文。推荐：64。",
        numCtx: "上下文窗口大小",
        numCtxHelp: "推理的最大上下文长度。较大的窗口允许更多上下文，但使用更多内存。",
      },
    },
    dataFiles: {
      title: "训练数据",
      empty: "暂无数据文件",
      dropzone: "将 JSONL 文件拖放到此处或点击上传",
      dropzoneActive: "将文件放在此处...",
      uploadButton: "上传文件",
      uploading: "上传中...",
      deleteConfirm: "删除此文件？",
      previewRowCount: "共 {{count}} 行",
      previewTruncated: "显示前 {{count}} 行",
      previewEmpty: "此文件不包含数据行。",
      previewError: "无法加载文件内容。",
      invalidRow: "无效行",
      showRawContent: "显示原始内容 ({{size}})",
      rawContentTitle: "原始内容 - 第 {{line}} 行",
      rawContentLength: "{{count}} 个字符",
      errorCount: "发现 {{count}} 个无效行",
      fileStatus: {
        pending: "等待处理",
        in_progress: "加载中...",
        completed: "已加载 {{loaded}} 行（跳过 {{skipped}} 行）",
        failed: "加载失败",
        skipped: "已跳过",
      },
      validationErrors: {
        INVALID_JSON: "无效的 JSON 语法",
        NOT_OBJECT: "必须是 JSON 对象",
        MISSING_INSTRUCTION: "缺少 \"instruction\" 字段",
        MISSING_OUTPUT: "缺少 \"output\" 字段",
        INVALID_INSTRUCTION_TYPE: "\"instruction\" 必须是字符串",
        INVALID_OUTPUT_TYPE: "\"output\" 必须是字符串",
      },
    },
    training: {
      title: "训练",
      startButton: "创建模型",
      cancelButton: "取消训练",
      noModel: "请先选择模型",
      noDataFiles: "请至少添加一个数据文件",
      readyDescription: "一切就绪。点击上方按钮开始创建模型。",
      status: {
        idle: "就绪",
        starting: "启动中...",
        loading_data: "加载数据...",
        loading_model: "加载模型...",
        training: "训练中...",
        exporting: "导出模型...",
        converting: "转换为 GGUF...",
        completed: "已完成",
        failed: "失败",
        cancelled: "已取消",
      },
      progress: "进度",
      step: "步骤 {{current}}/{{total}}",
      device: "设备",
      taskList: "任务",
      tasks: {
        detect_device: "检测计算设备",
        import_libraries: "导入 ML 库",
        load_model: "加载基础模型",
        setup_lora: "设置 LoRA 适配器",
        tokenize: "加载和标记化数据",
        train: "训练模型",
        merge_lora: "将 LoRA 与基础模型合并",
        convert_gguf: "转换为 GGUF 格式",
        create_modelfile: "创建 Ollama Modelfile",
        register_ollama: "在 Ollama 中注册模型",
      },
      taskWarnings: "跳过了 {{count}} 个无效行",
      stillWorking: "仍在处理...",
      errorTitle: "训练失败",
      completed: "训练完成",
      completedDescription: "模型创建成功。请检查输出文件夹中的 Modelfile。",
      cancelled: "训练已取消。",
    },
    ollama: {
      title: "Ollama 集成",
      runButton: "在 Ollama 中运行",
      modelName: "模型名称",
      running: "打开终端...",
    },
    presets: {
      title: "训练预设",
      description: "针对不同用例优化的快速启动配置。选择预设以应用其设置。",
      applyButton: "应用",
      applyConfirmTitle: "应用预设？",
      applyConfirmDescription: "这将用 \"{{preset}}\" 预设值覆盖您当前的训练、LoRA 和量化设置。此操作无法撤销。",
      recommended: "推荐",
      allModels: "所有模型",
      balanced: {
        name: "平衡",
        description: "大多数任务的速度和质量的良好平衡",
        pros: {
          versatile: "适用于大多数模型和数据",
          stable: "使用经过验证的默认值稳定训练",
          good_defaults: "实验的良好起点",
        },
        cons: {
          not_specialized: "未针对特定用例优化",
          moderate_time: "中等训练时间",
        },
      },
      chat: {
        name: "聊天/对话",
        description: "针对对话 AI 和指令遵循优化",
        pros: {
          natural_responses: "更自然的对话响应",
          instruction_following: "更好的指令遵循",
          diverse_outputs: "更多样化和创意的输出",
        },
        cons: {
          more_memory: "更高的内存使用",
          longer_training: "更长的训练时间",
        },
      },
      code: {
        name: "代码生成",
        description: "针对编程和代码补全优化",
        pros: {
          precise_syntax: "精确的语法学习",
          low_dropout: "低 dropout 保证精度",
          all_modules: "针对所有相关层",
        },
        cons: {
          more_memory: "更高的内存使用",
          slower_training: "较慢的训练速度",
        },
      },
      fast: {
        name: "快速迭代",
        description: "快速训练用于快速实验",
        pros: {
          quick_results: "快速测试结果",
          low_memory: "较低的内存需求",
          rapid_testing: "理想的快速原型设计",
        },
        cons: {
          lower_quality: "较低的输出质量",
          less_learning: "学习不够深入",
        },
      },
      high_quality: {
        name: "高质量",
        description: "以训练时间换取最高质量",
        pros: {
          best_results: "最佳可能结果",
          thorough_learning: "更多轮次的深入训练",
          all_modules: "全面的层覆盖",
        },
        cons: {
          long_training: "长训练时间",
          high_memory: "高内存需求",
          needs_gpu: "需要强大的 GPU",
        },
      },
      low_memory: {
        name: "低内存",
        description: "有限硬件的最小 VRAM 使用",
        pros: {
          minimal_vram: "最小 VRAM 使用",
          works_on_consumer: "可在消费级 GPU 上运行",
          gradient_accumulation: "有效的梯度累积",
        },
        cons: {
          slower_training: "较慢的训练速度",
          smaller_rank: "较小的 LoRA 秩限制能力",
        },
      },
      multilingual: {
        name: "多语言",
        description: "针对多语言模型优化",
        pros: {
          language_diversity: "保留语言多样性",
          balanced_learning: "平衡的跨语言学习",
          longer_warmup: "语言适应的延长预热",
        },
        cons: {
          needs_diverse_data: "需要多样化的训练数据",
          moderate_time: "中等训练时间",
        },
      },
      reasoning: {
        name: "推理/数学",
        description: "针对逻辑推理和数学优化",
        pros: {
          precise_learning: "精确和细致的学习",
          low_dropout: "低 dropout 保证一致性",
          consistent_outputs: "更一致的输出",
        },
        cons: {
          more_epochs: "需要更多训练轮次",
          higher_rank: "更高的秩增加内存",
        },
      },
    },
    errors: {
      ERR_PROJECT_1001: "已存在同名项目。",
      ERR_PROJECT_1002: "项目未找到。",
      ERR_PROJECT_1003: "无效的项目名称。",
      ERR_PROJECT_1004: "项目名称不能为空。",
      ERR_PROJECT_1005: "创建项目失败。",
      ERR_PROJECT_1006: "删除项目失败。",
      ERR_PROJECT_1007: "更新项目失败。",
      ERR_PROJECT_1008: "打开项目文件夹失败。",
      ERR_MODEL_2001: "读取模型配置失败。",
      ERR_MODEL_2002: "写入模型配置失败。",
      ERR_DATA_3001: "数据文件未找到。",
      ERR_DATA_3002: "无效的文件类型。仅允许 JSONL 文件。",
      ERR_DATA_3003: "上传文件失败。",
      ERR_DATA_3004: "删除文件失败。",
      ERR_DATA_3005: "读取数据文件失败。",
      ERR_TRAINING_4001: "训练任务已在运行。",
      ERR_TRAINING_4002: "没有正在运行的训练任务。",
      ERR_TRAINING_4003: "请在开始训练前添加数据文件。",
      ERR_TRAINING_4004: "未找到训练数据文件。",
      ERR_TRAINING_4005: "加载模型失败。",
      ERR_TRAINING_4006: "训练失败。",
      ERR_TRAINING_4007: "导出模型失败。",
      ERR_TRAINING_4008: "训练已取消。",
      ERR_TRAINING_4009: "未找到 llama.cpp。请先安装。",
      ERR_HF_5001: "未登录 Hugging Face。",
      ERR_HF_5002: "Hugging Face 登录失败。",
      ERR_HF_5003: "无效的 Hugging Face 令牌。",
      ERR_OLLAMA_6001: "Ollama 未安装。",
      ERR_OLLAMA_6002: "Ollama 未运行。",
      ERR_OLLAMA_6003: "在 Ollama 中创建模型失败。",
      ERR_OLLAMA_6004: "在 Ollama 中未找到模型。",
      ERR_OLLAMA_6005: "未找到 Modelfile。请先训练模型。",
      ERR_OLLAMA_6006: "打开终端失败。",
      ERR_OLLAMA_6007: "未配置基础模型。请先选择模型。",
      ERR_LLM_8001: "无效的 API 密钥格式。",
      ERR_LLM_8002: "API 密钥被拒绝。请检查您的密钥。",
      ERR_LLM_8003: "未知提供商。",
      ERR_LLM_8004: "无法访问提供商 API。",
      ERR_LLM_8005: "保存 API 密钥失败。",
      ERR_DATA_SOURCE_9001: "数据源的文件类型无效。",
      ERR_DATA_SOURCE_9002: "文件太大。",
      ERR_DATA_SOURCE_9003: "数据源为空。",
      ERR_GENERATION_9101: "LLM 提供商未配置。",
      ERR_GENERATION_9102: "模型不可用。",
      ERR_GENERATION_9103: "令牌限制超出。请使用较小的数据源。",
      ERR_GENERATION_9104: "与 LLM API 通信错误。",
      ERR_GENERATION_9105: "LLM 响应无效。",
      ERR_GENERATION_9106: "超出速率限制。请稍后重试。",
      ERR_SAVE_9201: "无效的文件名。",
      ERR_SAVE_9202: "保存文件失败。",
      unknown: "发生意外错误。",
    },
    validation: {
      mustBeInteger: "必须是整数",
      mustBeNumber: "必须是有效数字",
      mustBeString: "必须是文本",
      mustBeBoolean: "必须是 true 或 false",
      mustBeArray: "必须是列表",
      mustBeGreaterThan: "必须大于 {{min}}",
      mustBeAtLeast: "必须至少为 {{min}}",
      mustBeAtMost: "必须最多为 {{max}}",
      maxLength: "最多 {{maxLength}} 个字符",
    },
  },
};

export default zh;
