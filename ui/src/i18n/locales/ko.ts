// OllaForge - A web application that simplifies training LLMs with your own data for use in Ollama.
// Copyright (C) 2026  Marcel Joachim Kloubert (marcel@kloubert.dev)
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, either version 3 of the
// License, or (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

import type { TranslationSchema } from "../types";

const ko: TranslationSchema = {
  translation: {
    common: {
      loading: "로딩 중...",
      error: "오류가 발생했습니다",
      retry: "재시도",
      cancel: "취소",
      save: "저장",
      delete: "삭제",
      create: "생성",
      back: "뒤로",
      name: "이름",
      actions: "작업",
      optional: "선택사항",
      edit: "편집",
      ok: "확인",
      dismiss: "닫기",
    },
    app: {
      title: "OllaForge",
    },
    nav: {
      home: "홈",
      projects: "프로젝트",
    },
    theme: {
      light: "라이트",
      dark: "다크",
      system: "시스템",
      toggle: "테마 전환",
    },
    language: {
      en: "영어",
      de: "독일어",
      es: "스페인어",
      fr: "프랑스어",
      pt: "포르투갈어",
      uk: "우크라이나어",
      zh: "중국어",
      ja: "일본어",
      ko: "한국어",
      ar: "아랍어",
      hi: "힌디어",
      it: "이탈리아어",
      nl: "네덜란드어",
      pl: "폴란드어",
      select: "언어 선택",
    },
    api: {
      status: "API 상태",
      connected: "연결됨",
      disconnected: "연결 안 됨",
      checking: "확인 중...",
    },
    huggingface: {
      status: {
        loggedIn: "{{username}}(으)로 로그인됨",
        loggedOut: "로그인되지 않음",
        checking: "확인 중...",
      },
      changeToken: "토큰 변경",
      login: {
        title: "Hugging Face 로그인",
        description: "보호된 모델에 액세스하려면 Hugging Face 액세스 토큰을 입력하세요.",
        tokenLabel: "액세스 토큰",
        tokenPlaceholder: "hf_...",
        help: "보호된 모델을 다운로드하려면 Hugging Face 액세스 토큰이 필요합니다.",
        getToken: "여기서 토큰 받기",
        submit: "로그인",
        submitting: "로그인 중...",
        success: "로그인 성공!",
      },
      errors: {
        loginFailed: "로그인 실패. 토큰을 확인하세요.",
        invalidToken: "토큰 형식이 올바르지 않습니다. 토큰은 'hf_'로 시작해야 합니다.",
      },
    },
    llmProviders: {
      title: "LLM 제공자",
      status: {
        allValid: "모든 제공자 구성됨",
        someInvalid: "일부 제공자 미구성",
        noneConfigured: "구성된 제공자 없음",
        checking: "확인 중...",
        valid: "유효",
        invalid: "미구성",
      },
      providers: {
        openai: "OpenAI",
        anthropic: "Anthropic (Claude)",
        mistral: "Mistral",
      },
      login: {
        title: "{{provider}} 구성",
        description: "{{provider}} API 키를 입력하여 통합을 활성화하세요.",
        tokenLabel: "API 키",
        help: "{{provider}} 기능을 사용하려면 API 키가 필요합니다.",
        getToken: "여기서 API 키 받기",
        submit: "저장",
        submitting: "저장 중...",
        success: "API 키가 성공적으로 저장되었습니다!",
      },
      changeToken: "API 키 변경",
      errors: {
        loginFailed: "API 키 저장 실패. 키를 확인하세요.",
      },
    },
    generateFromSources: {
      title: "소스에서 생성",
      button: "소스에서 생성",
      buttonDisabled: "LLM 제공자가 구성되지 않음",
      sources: {
        title: "데이터 소스",
        uploadFile: "파일 업로드",
        uploadHint: "드래그 앤 드롭하거나 클릭하여 업로드",
        addText: "텍스트 추가",
        textPlaceholder: "여기에 텍스트를 붙여넣기...",
        empty: "아직 소스가 추가되지 않음",
        estimatedTokens: "약 {{tokens}} 토큰",
        totalTokens: "총: 약 {{tokens}} 토큰",
        sourcesCount: "{{count}}개 소스",
        acceptedFormats: "지원 형식: .txt, .md, .html, .json, .csv, .xml",
        tokenLimitWarning: "토큰 수가 컨텍스트 창 제한을 초과합니다 ({{limit}} 토큰)",
      },
      llm: {
        selectModel: "모델 선택...",
        contextInfo: "컨텍스트: {{context}}K 토큰",
        generate: "생성",
        generating: "생성 중...",
        progress: "청크 {{current}}/{{total}} 처리 중...",
        targetLanguage: "출력 언어",
        languages: {
          auto: "입력과 동일",
          en: "영어",
          de: "독일어",
          es: "스페인어",
          fr: "프랑스어",
          pt: "포르투갈어",
          uk: "우크라이나어",
          zh: "중국어 (간체)",
          ja: "일본어",
          ko: "한국어",
          ar: "아랍어",
          hi: "힌디어",
          it: "이탈리아어",
          nl: "네덜란드어",
          pl: "폴란드어",
        },
      },
      results: {
        title: "생성된 데이터",
        empty: "왼쪽 패널을 사용하여 데이터 생성",
        instruction: "지시",
        output: "출력",
        addRow: "행 추가",
        deleteRow: "삭제",
        validRows: "{{valid}}/{{total}} 행 유효",
        generated: "{{count}}개의 훈련 데이터 행 생성됨",
      },
      save: {
        title: "JSONL로 저장",
        filename: "파일 이름",
        save: "저장",
        saving: "저장 중...",
        success: "파일 {{filename}}이(가) 성공적으로 저장됨",
      },
      errors: {
        emptyCell: "셀은 비어 있을 수 없습니다",
        invalidFilename: "잘못된 파일 이름",
        noData: "저장할 데이터가 없습니다",
        invalidRows: "먼저 잘못된 행을 수정하세요",
      },
    },
    projects: {
      title: "프로젝트",
      empty: "아직 프로젝트가 없습니다",
      emptyDescription: "첫 번째 프로젝트를 만들어 시작하세요.",
      createNew: "새 프로젝트",
      createTitle: "새 프로젝트 만들기",
      createDescription: "새 프로젝트의 이름을 입력하세요.",
      namePlaceholder: "내 프로젝트",
      nameLabel: "프로젝트 이름",
      descriptionLabel: "설명",
      descriptionPlaceholder: "프로젝트에 대한 간단한 설명",
      creating: "생성 중...",
      saving: "저장 중...",
      openProject: "프로젝트 열기",
      editTitle: "프로젝트 편집",
      editDescription: "프로젝트 이름과 설명을 업데이트합니다.",
      deleteTitle: "프로젝트 삭제",
      deleteDescription: "\"{{name}}\"을(를) 삭제하시겠습니까? 이 작업은 되돌릴 수 없습니다.",
      deleting: "삭제 중...",
      openFolder: "폴더 열기",
    },
    project: {
      title: "프로젝트",
      backToProjects: "프로젝트 목록으로",
      selectModel: "기본 모델",
      selectModelPlaceholder: "모델 선택...",
      targetName: "대상 모델 이름",
      targetNamePlaceholder: "사용자 정의 이름 입력...",
      configuration: "구성",
      status: "상태",
      tabs: {
        basic: "기본",
        advanced: "고급",
      },
      advancedPlaceholder: "고급 설정 곧 제공 예정...",
    },
    advancedConfig: {
      helpPanel: {
        title: "고급 설정 정보",
        description: "이 설정은 QLoRA(Quantized Low-Rank Adaptation)를 사용하여 모델을 훈련하는 방법을 제어합니다. QLoRA는 4비트 양자화와 저순위 어댑터를 사용하여 소비자용 하드웨어에서 대규모 언어 모델을 미세 조정할 수 있게 합니다. 기본값은 대부분의 사용 사례에 적합합니다 - 영향을 이해하는 경우에만 변경하세요.",
        learnMore: "자세히",
        links: {
          huggingface: "Hugging Face Transformers",
          qlora: "QLoRA 논문",
          lora: "LoRA 설명",
          transformers: "훈련 문서",
        },
      },
      trainingParams: {
        title: "훈련 매개변수",
        numEpochs: "에포크",
        numEpochsHelp: "훈련 패스 수. 더 많은 에포크는 품질을 향상시킬 수 있지만 과적합 위험이 있습니다. 권장: 1-5.",
        batchSize: "배치 크기",
        batchSizeHelp: "훈련 단계당 샘플 수. 더 큰 크기는 더 빠르게 훈련되지만 더 많은 메모리가 필요합니다. 권장: 1-8.",
        gradientAccumulation: "그래디언트 누적",
        gradientAccumulationHelp: "여러 단계에 걸쳐 그래디언트를 누적합니다. 더 적은 메모리로 더 큰 배치 크기를 시뮬레이션합니다.",
        learningRate: "학습률",
        learningRateHelp: "가중치 업데이트의 단계 크기. 너무 높으면 불안정해지고, 너무 낮으면 훈련이 느려집니다. 권장: 1e-5에서 5e-4.",
        warmupRatio: "워밍업 비율",
        warmupRatioHelp: "학습률을 점진적으로 증가시키는 훈련의 비율. 초기 훈련 안정화에 도움이 됩니다.",
        maxLength: "최대 토큰 길이",
        maxLengthHelp: "훈련의 최대 시퀀스 길이. 더 긴 시퀀스는 더 많은 메모리가 필요합니다.",
        fp16: "FP16 (반정밀도)",
        fp16Help: "더 빠른 훈련을 위해 16비트 부동 소수점 사용. CUDA GPU에서만 사용 가능.",
        optimizer: "옵티마이저",
        optimizerHelp: "가중치 업데이트 알고리즘. paged_adamw_8bit는 QLoRA에 메모리 효율적입니다.",
        optimizers: {
          paged_adamw_8bit: "Paged AdamW 8-bit (GPU 권장)",
          adamw_torch: "AdamW (CPU 권장)",
          adamw_hf: "AdamW (Hugging Face)",
          sgd: "SGD",
        },
        weightDecay: "가중치 감쇠",
        weightDecayHelp: "과적합 방지를 위한 L2 정규화. 더 높은 값은 더 많은 정규화를 추가합니다. 권장: 0.01.",
        maxGradNorm: "최대 그래디언트 노름",
        maxGradNormHelp: "그래디언트 클리핑의 최대 그래디언트 노름. 그래디언트 폭발을 방지합니다. 권장: 1.0.",
        lrScheduler: "LR 스케줄러",
        lrSchedulerHelp: "학습률 스케줄러 전략. 훈련 중 학습률 변화를 제어합니다.",
        schedulers: {
          linear: "선형 (권장)",
          cosine: "코사인",
          constant: "상수",
          polynomial: "다항식",
        },
        neftuneNoise: "NEFTune 노이즈 알파",
        neftuneNoiseHelp: "훈련 중 임베딩에 노이즈 추가. 일반화를 개선할 수 있습니다. 0 = 비활성화, 활성화 시 5-15 권장.",
        seed: "랜덤 시드",
        seedHelp: "재현성을 위한 시드. 동일한 시드를 사용하면 훈련 실행 간에 동일한 결과를 얻습니다.",
        bf16: "BF16 (Brain Float 16)",
        bf16Help: "fp16 대신 bfloat16 정밀도 사용. Ampere+ GPU(RTX 3000+)에서만 사용 가능. fp16보다 더 나은 수치적 안정성.",
        loggingSteps: "로깅 단계",
        loggingStepsHelp: "N 단계마다 훈련 메트릭 로깅. 낮은 값은 더 빈번한 업데이트를 제공하지만 훈련을 느리게 할 수 있습니다.",
        saveStrategy: "저장 전략",
        saveStrategyHelp: "훈련 중 모델 체크포인트를 저장할 시기.",
        saveStrategies: {
          no: "체크포인트 없음",
          epoch: "각 에포크 후 (권장)",
          steps: "N 단계마다",
        },
      },
      defaults: {
        showDefaults: "기본값 사용 중",
        cudaDefault: "GPU 기본값: {{value}}",
        cpuDefault: "CPU 기본값: {{value}}",
        resetToDefaults: "기본값으로 재설정",
        resetConfirm: "이 섹션의 모든 값이 기본값으로 재설정됩니다.",
      },
      loraParams: {
        title: "LoRA 구성",
        rank: "랭크 (r)",
        rankHelp: "저순위 행렬의 차원. 높은 값은 더 많은 정보를 캡처하지만 더 많은 메모리를 사용하고 과적합 위험이 있습니다. 권장: 8-64.",
        alpha: "알파",
        alphaHelp: "LoRA 가중치의 스케일링 팩터. 일반적으로 랭크의 2배로 설정. 높은 값은 미세 조정의 영향을 증가시킵니다.",
        dropout: "드롭아웃",
        dropoutHelp: "LoRA 레이어의 드롭아웃 확률. 과적합 방지에 도움이 됩니다. 권장: 0.05-0.1.",
        targetModules: "대상 모듈",
        targetModulesHelp: "LoRA를 적용할 모델 레이어. 더 많은 모듈 = 더 많은 미세 조정 용량이지만 더 많은 메모리 사용.",
        modules: {
          q_proj: "쿼리 프로젝션 (q_proj)",
          k_proj: "키 프로젝션 (k_proj)",
          v_proj: "값 프로젝션 (v_proj)",
          o_proj: "출력 프로젝션 (o_proj)",
          gate_proj: "게이트 프로젝션 (gate_proj)",
          up_proj: "업 프로젝션 (up_proj)",
          down_proj: "다운 프로젝션 (down_proj)",
        },
        bias: "바이어스 훈련",
        biasHelp: "훈련 중 바이어스 항 처리 방법. 'none'은 바이어스 고정, 'lora_only'는 LoRA 바이어스 훈련, 'all'은 모든 바이어스 훈련.",
        biasOptions: {
          none: "없음 (바이어스 고정)",
          lora_only: "LoRA만",
          all: "모든 바이어스",
        },
        useRslora: "RSLoRA 사용",
        useRsloraHelp: "Rank-Stabilized LoRA는 높은 랭크(r >= 64)에서 훈련 안정성과 성능을 향상시킵니다. 큰 랭크에 권장.",
        useDora: "DoRA 사용 (실험적)",
        useDoraHelp: "Weight-Decomposed Low-Rank Adaptation은 미세 조정 품질을 향상시킬 수 있습니다. 실험적 기능, 훈련 시간이 증가할 수 있습니다.",
        modulesToSave: "저장할 모듈",
        modulesToSaveHelp: "완전히 훈련할 추가 모듈 (LoRA 아님). lm_head와 같은 출력 레이어 훈련에 유용.",
        saveModules: {
          lm_head: "언어 모델 헤드 (lm_head)",
          embed_tokens: "토큰 임베딩 (embed_tokens)",
        },
      },
      quantizationParams: {
        title: "양자화",
        loadIn4bit: "4비트로 로드",
        loadIn4bitHelp: "메모리 사용량 감소를 위해 4비트 정밀도로 모델 가중치 로드. 제한된 GPU 메모리에서 QLoRA에 필요. CUDA GPU에서만 사용 가능.",
        quantType: "4비트 양자화 유형",
        quantTypeHelp: "4비트 양자화 알고리즘. NF4(Normal Float 4)는 더 나은 정확도를 위해 권장됩니다.",
        quantTypes: {
          nf4: "NF4 (권장)",
          fp4: "FP4",
        },
        doubleQuant: "이중 양자화",
        doubleQuantHelp: "메모리를 더 줄이기 위해 이차 양자화 적용. 작은 정확도 트레이드오프로 상당한 메모리 절약.",
        computeDtype: "계산 데이터 유형",
        computeDtypeHelp: "훈련 중 계산을 위한 데이터 유형. bfloat16은 Ampere+ GPU(RTX 3000+)에서 더 나은 수치적 안정성을 제공합니다.",
        computeDtypes: {
          float16: "Float16 (권장)",
          bfloat16: "BFloat16 (RTX 3000+)",
          float32: "Float32 (전체 정밀도)",
        },
        outputQuantization: "출력 양자화",
        outputQuantizationHelp: "최종 GGUF 모델의 양자화 형식. q8_0은 크기와 품질 사이의 좋은 균형을 제공합니다.",
        outputTypes: {
          f32: "F32 (전체 정밀도, 가장 큼)",
          f16: "F16 (반정밀도)",
          bf16: "BF16 (brain float 16)",
          q8_0: "Q8_0 (8비트, 권장)",
          auto: "자동 (llama.cpp에 맡김)",
        },
        cudaOnly: "이 설정은 CUDA GPU에서 훈련할 때만 적용됩니다.",
      },
      modelfileParams: {
        title: "Ollama Modelfile",
        temperature: "온도",
        temperatureHelp: "출력의 무작위성을 제어합니다. 낮은 값은 응답을 더 집중적이고 결정론적으로 만들고, 높은 값은 더 창의적으로 만듭니다. 권장: 0.7-0.9.",
        topP: "Top P (핵 샘플링)",
        topPHelp: "이 값까지의 누적 확률을 가진 토큰만 고려합니다. 낮은 값은 더 가능성 있는 토큰에 집중합니다. 권장: 0.9.",
        topK: "Top K",
        topKHelp: "토큰 선택을 가장 가능성 있는 K개 옵션으로 제한합니다. 낮은 값은 더 집중적입니다. 권장: 40.",
        system: "시스템 프롬프트",
        systemHelp: "모델 동작 방식을 정의하는 지침. 모델의 성격과 능력을 설정합니다.",
        systemPlaceholder: "당신은 도움이 되는 어시스턴트입니다.",
        stop: "중지 시퀀스",
        stopHelp: "모델에 생성 중지를 신호하는 시퀀스. 여러 중지 시퀀스를 추가할 수 있습니다.",
        stopPlaceholder: "중지 시퀀스 입력...",
        stopAdd: "추가",
        repeatPenalty: "반복 페널티",
        repeatPenaltyHelp: "토큰 반복에 대한 페널티. 높은 값은 반복을 줄입니다. 권장: 1.1.",
        repeatLastN: "마지막 N 반복 확인",
        repeatLastNHelp: "반복 확인을 위한 토큰 수. 높은 값은 더 많은 컨텍스트를 고려합니다. 권장: 64.",
        numCtx: "컨텍스트 창 크기",
        numCtxHelp: "추론의 최대 컨텍스트 길이. 더 큰 창은 더 많은 컨텍스트를 허용하지만 더 많은 메모리를 사용합니다.",
      },
    },
    dataFiles: {
      title: "훈련 데이터",
      empty: "아직 데이터 파일이 없습니다",
      dropzone: "여기에 JSONL 파일을 드롭하거나 클릭하여 업로드",
      dropzoneActive: "여기에 파일 드롭...",
      uploadButton: "파일 업로드",
      uploading: "업로드 중...",
      deleteConfirm: "이 파일을 삭제하시겠습니까?",
      previewRowCount: "총 {{count}}행",
      previewTruncated: "처음 {{count}}행 표시",
      previewEmpty: "이 파일에는 데이터 행이 없습니다.",
      previewError: "파일 내용을 로드할 수 없습니다.",
      invalidRow: "잘못된 행",
      showRawContent: "원본 표시 ({{size}})",
      rawContentTitle: "원본 내용 - 행 {{line}}",
      rawContentLength: "{{count}}자",
      errorCount: "{{count}}개의 잘못된 행 발견",
      fileStatus: {
        pending: "처리 대기 중",
        in_progress: "로딩 중...",
        completed: "{{loaded}}행 로드됨 ({{skipped}}개 건너뜀)",
        failed: "로드 실패",
        skipped: "건너뜀",
      },
      validationErrors: {
        INVALID_JSON: "잘못된 JSON 구문",
        NOT_OBJECT: "JSON 객체여야 합니다",
        MISSING_INSTRUCTION: "\"instruction\" 필드 누락",
        MISSING_OUTPUT: "\"output\" 필드 누락",
        INVALID_INSTRUCTION_TYPE: "\"instruction\"은 문자열이어야 합니다",
        INVALID_OUTPUT_TYPE: "\"output\"은 문자열이어야 합니다",
      },
    },
    training: {
      title: "훈련",
      startButton: "모델 생성",
      cancelButton: "훈련 취소",
      noModel: "먼저 모델을 선택하세요",
      noDataFiles: "하나 이상의 데이터 파일을 추가하세요",
      readyDescription: "모든 준비가 완료되었습니다. 위의 버튼을 클릭하여 모델 생성을 시작하세요.",
      status: {
        idle: "준비 완료",
        starting: "시작 중...",
        loading_data: "데이터 로딩 중...",
        loading_model: "모델 로딩 중...",
        training: "훈련 중...",
        exporting: "모델 내보내기 중...",
        converting: "GGUF로 변환 중...",
        completed: "완료",
        failed: "실패",
        cancelled: "취소됨",
      },
      progress: "진행률",
      step: "단계 {{current}}/{{total}}",
      device: "장치",
      taskList: "작업",
      tasks: {
        detect_device: "계산 장치 감지",
        import_libraries: "ML 라이브러리 가져오기",
        load_model: "기본 모델 로드",
        setup_lora: "LoRA 어댑터 설정",
        tokenize: "데이터 로드 및 토큰화",
        train: "모델 훈련",
        merge_lora: "LoRA를 기본 모델에 병합",
        convert_gguf: "GGUF 형식으로 변환",
        create_modelfile: "Ollama Modelfile 생성",
        register_ollama: "Ollama에 모델 등록",
      },
      taskWarnings: "{{count}}개의 잘못된 행 건너뜀",
      stillWorking: "아직 작업 중...",
      errorTitle: "훈련 실패",
      completed: "훈련 완료",
      completedDescription: "모델이 성공적으로 생성되었습니다. 출력 폴더에서 Modelfile을 확인하세요.",
      cancelled: "훈련이 취소되었습니다.",
    },
    ollama: {
      title: "Ollama 통합",
      runButton: "Ollama에서 실행",
      modelName: "모델 이름",
      running: "터미널 열기...",
    },
    presets: {
      title: "훈련 프리셋",
      description: "다양한 사용 사례에 최적화된 빠른 시작 구성. 프리셋을 선택하여 설정을 적용합니다.",
      applyButton: "적용",
      applyConfirmTitle: "프리셋을 적용하시겠습니까?",
      applyConfirmDescription: "현재 훈련, LoRA 및 양자화 설정이 \"{{preset}}\" 프리셋 값으로 덮어쓰여집니다. 이 작업은 되돌릴 수 없습니다.",
      recommended: "권장",
      allModels: "모든 모델",
      balanced: {
        name: "균형",
        description: "대부분의 작업에 대한 속도와 품질의 좋은 균형",
        pros: {
          versatile: "대부분의 모델과 데이터에서 잘 작동",
          stable: "검증된 기본값으로 안정적인 훈련",
          good_defaults: "실험을 위한 좋은 시작점",
        },
        cons: {
          not_specialized: "특정 사용 사례에 최적화되지 않음",
          moderate_time: "보통의 훈련 시간",
        },
      },
      chat: {
        name: "채팅/대화",
        description: "대화형 AI 및 지시 따르기에 최적화",
        pros: {
          natural_responses: "더 자연스러운 대화 응답",
          instruction_following: "더 나은 지시 따르기",
          diverse_outputs: "더 다양하고 창의적인 출력",
        },
        cons: {
          more_memory: "더 높은 메모리 사용",
          longer_training: "더 긴 훈련 시간",
        },
      },
      code: {
        name: "코드 생성",
        description: "프로그래밍 및 코드 완성에 최적화",
        pros: {
          precise_syntax: "정밀한 구문 학습",
          low_dropout: "정확도를 위한 낮은 드롭아웃",
          all_modules: "모든 관련 레이어 대상",
        },
        cons: {
          more_memory: "더 높은 메모리 사용",
          slower_training: "더 느린 훈련 속도",
        },
      },
      fast: {
        name: "빠른 반복",
        description: "빠른 실험을 위한 빠른 훈련",
        pros: {
          quick_results: "테스트를 위한 빠른 결과",
          low_memory: "낮은 메모리 요구사항",
          rapid_testing: "빠른 프로토타이핑에 이상적",
        },
        cons: {
          lower_quality: "낮은 출력 품질",
          less_learning: "덜 철저한 학습",
        },
      },
      high_quality: {
        name: "고품질",
        description: "훈련 시간을 희생하여 최대 품질",
        pros: {
          best_results: "가능한 최고의 결과",
          thorough_learning: "더 많은 에포크에 걸친 철저한 훈련",
          all_modules: "포괄적인 레이어 커버리지",
        },
        cons: {
          long_training: "긴 훈련 시간",
          high_memory: "높은 메모리 요구사항",
          needs_gpu: "강력한 GPU 필요",
        },
      },
      low_memory: {
        name: "낮은 메모리",
        description: "제한된 하드웨어를 위한 최소 VRAM 사용",
        pros: {
          minimal_vram: "최소 VRAM 사용",
          works_on_consumer: "소비자용 GPU에서 작동",
          gradient_accumulation: "효과적인 그래디언트 누적",
        },
        cons: {
          slower_training: "더 느린 훈련 속도",
          smaller_rank: "작은 LoRA 랭크가 용량 제한",
        },
      },
      multilingual: {
        name: "다국어",
        description: "다국어 모델에 최적화",
        pros: {
          language_diversity: "언어 다양성 보존",
          balanced_learning: "균형 잡힌 교차 언어 학습",
          longer_warmup: "언어 적응을 위한 연장된 워밍업",
        },
        cons: {
          needs_diverse_data: "다양한 훈련 데이터 필요",
          moderate_time: "보통의 훈련 시간",
        },
      },
      reasoning: {
        name: "추론/수학",
        description: "논리적 추론과 수학에 최적화",
        pros: {
          precise_learning: "정밀하고 신중한 학습",
          low_dropout: "일관성을 위한 낮은 드롭아웃",
          consistent_outputs: "더 일관된 출력",
        },
        cons: {
          more_epochs: "더 많은 훈련 에포크 필요",
          higher_rank: "높은 랭크가 메모리 증가",
        },
      },
    },
    errors: {
      ERR_PROJECT_1001: "이 이름의 프로젝트가 이미 존재합니다.",
      ERR_PROJECT_1002: "프로젝트를 찾을 수 없습니다.",
      ERR_PROJECT_1003: "잘못된 프로젝트 이름입니다.",
      ERR_PROJECT_1004: "프로젝트 이름은 비어 있을 수 없습니다.",
      ERR_PROJECT_1005: "프로젝트 생성에 실패했습니다.",
      ERR_PROJECT_1006: "프로젝트 삭제에 실패했습니다.",
      ERR_PROJECT_1007: "프로젝트 업데이트에 실패했습니다.",
      ERR_PROJECT_1008: "프로젝트 폴더를 열 수 없습니다.",
      ERR_MODEL_2001: "모델 구성을 읽을 수 없습니다.",
      ERR_MODEL_2002: "모델 구성을 쓸 수 없습니다.",
      ERR_DATA_3001: "데이터 파일을 찾을 수 없습니다.",
      ERR_DATA_3002: "잘못된 파일 유형입니다. JSONL 파일만 허용됩니다.",
      ERR_DATA_3003: "파일 업로드에 실패했습니다.",
      ERR_DATA_3004: "파일 삭제에 실패했습니다.",
      ERR_DATA_3005: "데이터 파일을 읽을 수 없습니다.",
      ERR_TRAINING_4001: "훈련 작업이 이미 실행 중입니다.",
      ERR_TRAINING_4002: "실행 중인 훈련 작업이 없습니다.",
      ERR_TRAINING_4003: "훈련을 시작하기 전에 데이터 파일을 추가하세요.",
      ERR_TRAINING_4004: "훈련 데이터 파일을 찾을 수 없습니다.",
      ERR_TRAINING_4005: "모델 로드에 실패했습니다.",
      ERR_TRAINING_4006: "훈련에 실패했습니다.",
      ERR_TRAINING_4007: "모델 내보내기에 실패했습니다.",
      ERR_TRAINING_4008: "훈련이 취소되었습니다.",
      ERR_TRAINING_4009: "llama.cpp를 찾을 수 없습니다. 먼저 설치하세요.",
      ERR_HF_5001: "Hugging Face에 로그인되지 않았습니다.",
      ERR_HF_5002: "Hugging Face 로그인에 실패했습니다.",
      ERR_HF_5003: "잘못된 Hugging Face 토큰입니다.",
      ERR_OLLAMA_6001: "Ollama가 설치되지 않았습니다.",
      ERR_OLLAMA_6002: "Ollama가 실행 중이지 않습니다.",
      ERR_OLLAMA_6003: "Ollama에서 모델 생성에 실패했습니다.",
      ERR_OLLAMA_6004: "Ollama에서 모델을 찾을 수 없습니다.",
      ERR_OLLAMA_6005: "Modelfile을 찾을 수 없습니다. 먼저 모델을 훈련하세요.",
      ERR_OLLAMA_6006: "터미널을 열 수 없습니다.",
      ERR_OLLAMA_6007: "기본 모델이 구성되지 않았습니다. 먼저 모델을 선택하세요.",
      ERR_LLM_8001: "잘못된 API 키 형식입니다.",
      ERR_LLM_8002: "API 키가 거부되었습니다. 키를 확인하세요.",
      ERR_LLM_8003: "알 수 없는 제공자입니다.",
      ERR_LLM_8004: "제공자 API에 연결할 수 없습니다.",
      ERR_LLM_8005: "API 키를 저장할 수 없습니다.",
      ERR_DATA_SOURCE_9001: "데이터 소스의 파일 유형이 잘못되었습니다.",
      ERR_DATA_SOURCE_9002: "파일이 너무 큽니다.",
      ERR_DATA_SOURCE_9003: "데이터 소스가 비어 있습니다.",
      ERR_GENERATION_9101: "LLM 제공자가 구성되지 않았습니다.",
      ERR_GENERATION_9102: "모델을 사용할 수 없습니다.",
      ERR_GENERATION_9103: "토큰 제한을 초과했습니다. 더 작은 데이터 소스를 사용하세요.",
      ERR_GENERATION_9104: "LLM API와 통신 오류입니다.",
      ERR_GENERATION_9105: "LLM의 잘못된 응답입니다.",
      ERR_GENERATION_9106: "속도 제한을 초과했습니다. 잠시 후 다시 시도하세요.",
      ERR_SAVE_9201: "잘못된 파일 이름입니다.",
      ERR_SAVE_9202: "파일 저장에 실패했습니다.",
      unknown: "예기치 않은 오류가 발생했습니다.",
    },
    validation: {
      mustBeInteger: "정수여야 합니다",
      mustBeNumber: "유효한 숫자여야 합니다",
      mustBeString: "텍스트여야 합니다",
      mustBeBoolean: "true 또는 false여야 합니다",
      mustBeArray: "목록이어야 합니다",
      mustBeGreaterThan: "{{min}}보다 커야 합니다",
      mustBeAtLeast: "최소 {{min}}이어야 합니다",
      mustBeAtMost: "최대 {{max}}이어야 합니다",
      maxLength: "최대 {{maxLength}}자",
    },
  },
};

export default ko;
