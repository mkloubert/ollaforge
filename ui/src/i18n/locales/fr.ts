// OllaForge - A web application that simplifies training LLMs with your own data for use in Ollama.
// Copyright (C) 2026  Marcel Joachim Kloubert (marcel@kloubert.dev)
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, either version 3 of the
// License, or (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

import type { TranslationSchema } from "../types";

const fr: TranslationSchema = {
  translation: {
    common: {
      loading: "Chargement...",
      error: "Une erreur s'est produite",
      retry: "Réessayer",
      cancel: "Annuler",
      save: "Enregistrer",
      delete: "Supprimer",
      create: "Créer",
      back: "Retour",
      name: "Nom",
      actions: "Actions",
      optional: "optionnel",
      edit: "Modifier",
      ok: "OK",
      dismiss: "Fermer",
    },
    app: {
      title: "OllaForge",
    },
    nav: {
      home: "Accueil",
      projects: "Projets",
    },
    theme: {
      light: "Clair",
      dark: "Sombre",
      system: "Système",
      toggle: "Changer de thème",
    },
    language: {
      en: "Anglais",
      de: "Allemand",
      es: "Espagnol",
      fr: "Français",
      pt: "Portugais",
      uk: "Ukrainien",
      zh: "Chinois",
      ja: "Japonais",
      ko: "Coréen",
      ar: "Arabe",
      hi: "Hindi",
      it: "Italien",
      nl: "Néerlandais",
      pl: "Polonais",
      select: "Sélectionner la langue",
    },
    api: {
      status: "État de l'API",
      connected: "Connecté",
      disconnected: "Déconnecté",
      checking: "Vérification...",
    },
    huggingface: {
      status: {
        loggedIn: "Connecté en tant que {{username}}",
        loggedOut: "Non connecté",
        checking: "Vérification...",
      },
      changeToken: "Changer le token",
      login: {
        title: "Connexion Hugging Face",
        description:
          "Entrez votre token d'accès Hugging Face pour accéder aux modèles protégés.",
        tokenLabel: "Token d'accès",
        tokenPlaceholder: "hf_...",
        help: "Vous avez besoin d'un token d'accès Hugging Face pour télécharger les modèles protégés.",
        getToken: "Obtenez votre token ici",
        submit: "Se connecter",
        submitting: "Connexion...",
        success: "Connexion réussie !",
      },
      errors: {
        loginFailed: "Échec de connexion. Veuillez vérifier votre token.",
        invalidToken: "Format de token invalide. Le token doit commencer par 'hf_'.",
      },
    },
    llmProviders: {
      title: "Fournisseurs LLM",
      status: {
        allValid: "Tous les fournisseurs configurés",
        someInvalid: "Certains fournisseurs non configurés",
        noneConfigured: "Aucun fournisseur configuré",
        checking: "Vérification...",
        valid: "Valide",
        invalid: "Non configuré",
      },
      providers: {
        openai: "OpenAI",
        anthropic: "Anthropic (Claude)",
        mistral: "Mistral",
      },
      login: {
        title: "Configurer {{provider}}",
        description: "Entrez votre clé API {{provider}} pour activer l'intégration.",
        tokenLabel: "Clé API",
        help: "Vous avez besoin d'une clé API pour utiliser les fonctionnalités {{provider}}.",
        getToken: "Obtenez votre clé API ici",
        submit: "Enregistrer",
        submitting: "Enregistrement...",
        success: "Clé API enregistrée avec succès !",
      },
      changeToken: "Changer la clé API",
      errors: {
        loginFailed: "Échec de l'enregistrement de la clé API. Veuillez vérifier votre clé.",
      },
    },
    generateFromSources: {
      title: "Générer à partir des sources",
      button: "Générer à partir des sources",
      buttonDisabled: "Aucun fournisseur LLM configuré",
      sources: {
        title: "Sources de données",
        uploadFile: "Télécharger un fichier",
        uploadHint: "Glisser-déposer ou cliquer pour télécharger",
        addText: "Ajouter du texte",
        textPlaceholder: "Collez votre texte ici...",
        empty: "Aucune source ajoutée",
        estimatedTokens: "~{{tokens}} tokens",
        totalTokens: "Total : ~{{tokens}} tokens",
        sourcesCount: "{{count}} source(s)",
        acceptedFormats: "Acceptés : .txt, .md, .html, .json, .csv, .xml",
        tokenLimitWarning: "Le nombre de tokens dépasse la limite de la fenêtre de contexte ({{limit}} tokens)",
      },
      llm: {
        selectModel: "Sélectionner un modèle...",
        contextInfo: "Contexte : {{context}}K tokens",
        generate: "Générer",
        generating: "Génération...",
        progress: "Traitement du fragment {{current}} sur {{total}}...",
        targetLanguage: "Langue de sortie",
        languages: {
          auto: "Identique à l'entrée",
          en: "Anglais",
          de: "Allemand",
          es: "Espagnol",
          fr: "Français",
          pt: "Portugais",
          uk: "Ukrainien",
          zh: "Chinois (Simplifié)",
          ja: "Japonais",
          ko: "Coréen",
          ar: "Arabe",
          hi: "Hindi",
          it: "Italien",
          nl: "Néerlandais",
          pl: "Polonais",
        },
      },
      results: {
        title: "Données générées",
        empty: "Générez des données à l'aide du panneau de gauche",
        instruction: "Instruction",
        output: "Sortie",
        addRow: "Ajouter une ligne",
        deleteRow: "Supprimer",
        validRows: "{{valid}} sur {{total}} lignes valides",
        generated: "{{count}} lignes de données d'entraînement générées",
      },
      save: {
        title: "Enregistrer en JSONL",
        filename: "Nom du fichier",
        save: "Enregistrer",
        saving: "Enregistrement...",
        success: "Fichier {{filename}} enregistré avec succès",
      },
      errors: {
        emptyCell: "La cellule ne peut pas être vide",
        invalidFilename: "Nom de fichier invalide",
        noData: "Aucune donnée à enregistrer",
        invalidRows: "Veuillez d'abord corriger les lignes invalides",
      },
    },
    projects: {
      title: "Projets",
      empty: "Aucun projet pour le moment",
      emptyDescription: "Créez votre premier projet pour commencer.",
      createNew: "Nouveau projet",
      createTitle: "Créer un nouveau projet",
      createDescription: "Entrez un nom pour votre nouveau projet.",
      namePlaceholder: "Mon projet",
      nameLabel: "Nom du projet",
      descriptionLabel: "Description",
      descriptionPlaceholder: "Une brève description de votre projet",
      creating: "Création...",
      saving: "Enregistrement...",
      openProject: "Ouvrir le projet",
      editTitle: "Modifier le projet",
      editDescription: "Mettez à jour le nom et la description du projet.",
      deleteTitle: "Supprimer le projet",
      deleteDescription:
        "Êtes-vous sûr de vouloir supprimer \"{{name}}\" ? Cette action ne peut pas être annulée.",
      deleting: "Suppression...",
      openFolder: "Ouvrir le dossier",
    },
    project: {
      title: "Projet",
      backToProjects: "Retour aux projets",
      selectModel: "Modèle de base",
      selectModelPlaceholder: "Sélectionner un modèle...",
      targetName: "Nom du modèle cible",
      targetNamePlaceholder: "Entrez un nom personnalisé...",
      configuration: "Configuration",
      status: "État",
      tabs: {
        basic: "Base",
        advanced: "Avancé",
      },
      advancedPlaceholder: "Paramètres avancés à venir...",
    },
    advancedConfig: {
      helpPanel: {
        title: "À propos des paramètres avancés",
        description: "Ces paramètres contrôlent comment votre modèle est entraîné avec QLoRA (Quantized Low-Rank Adaptation). QLoRA permet le fine-tuning de grands modèles de langage sur du matériel grand public en utilisant la quantification 4 bits et des adaptateurs de faible rang. Les valeurs par défaut fonctionnent bien pour la plupart des cas d'utilisation - ne les modifiez que si vous comprenez leur impact.",
        learnMore: "En savoir plus",
        links: {
          huggingface: "Hugging Face Transformers",
          qlora: "Article QLoRA",
          lora: "LoRA expliqué",
          transformers: "Documentation d'entraînement",
        },
      },
      trainingParams: {
        title: "Paramètres d'entraînement",
        numEpochs: "Époques",
        numEpochsHelp: "Nombre de passes d'entraînement. Plus d'époques peuvent améliorer la qualité mais risquent le surapprentissage. Recommandé : 1-5.",
        batchSize: "Taille du lot",
        batchSizeHelp: "Échantillons par étape d'entraînement. Des tailles plus grandes entraînent plus vite mais nécessitent plus de mémoire. Recommandé : 1-8.",
        gradientAccumulation: "Accumulation de gradient",
        gradientAccumulationHelp: "Accumule les gradients sur plusieurs étapes. Simule une taille de lot plus grande avec moins de mémoire.",
        learningRate: "Taux d'apprentissage",
        learningRateHelp: "Taille du pas pour les mises à jour des poids. Trop élevé cause de l'instabilité, trop bas ralentit l'entraînement. Recommandé : 1e-5 à 5e-4.",
        warmupRatio: "Ratio de préchauffage",
        warmupRatioHelp: "Fraction de l'entraînement pour augmenter progressivement le taux d'apprentissage. Aide à stabiliser le début de l'entraînement.",
        maxLength: "Longueur maximale des tokens",
        maxLengthHelp: "Longueur maximale de séquence pour l'entraînement. Les séquences plus longues nécessitent plus de mémoire.",
        fp16: "FP16 (Demi-précision)",
        fp16Help: "Utilise le point flottant 16 bits pour un entraînement plus rapide. Disponible uniquement sur les GPU CUDA.",
        optimizer: "Optimiseur",
        optimizerHelp: "Algorithme de mise à jour des poids. paged_adamw_8bit est efficace en mémoire pour QLoRA.",
        optimizers: {
          paged_adamw_8bit: "Paged AdamW 8-bit (recommandé pour GPU)",
          adamw_torch: "AdamW (recommandé pour CPU)",
          adamw_hf: "AdamW (Hugging Face)",
          sgd: "SGD",
        },
        weightDecay: "Décroissance des poids",
        weightDecayHelp: "Régularisation L2 pour prévenir le surapprentissage. Des valeurs plus élevées ajoutent plus de régularisation. Recommandé : 0.01.",
        maxGradNorm: "Norme maximale du gradient",
        maxGradNormHelp: "Norme maximale du gradient pour l'écrêtage. Prévient l'explosion des gradients. Recommandé : 1.0.",
        lrScheduler: "Planificateur LR",
        lrSchedulerHelp: "Stratégie du planificateur de taux d'apprentissage. Contrôle comment le taux d'apprentissage change pendant l'entraînement.",
        schedulers: {
          linear: "Linéaire (recommandé)",
          cosine: "Cosinus",
          constant: "Constant",
          polynomial: "Polynomial",
        },
        neftuneNoise: "NEFTune Noise Alpha",
        neftuneNoiseHelp: "Ajoute du bruit aux embeddings pendant l'entraînement. Peut améliorer la généralisation. 0 = désactivé, 5-15 recommandé si activé.",
        seed: "Graine aléatoire",
        seedHelp: "Graine pour la reproductibilité. Utilisez la même graine pour obtenir des résultats identiques entre les exécutions.",
        bf16: "BF16 (Brain Float 16)",
        bf16Help: "Utilise la précision bfloat16 au lieu de fp16. Disponible uniquement sur les GPU Ampere+ (RTX 3000+). Meilleure stabilité numérique que fp16.",
        loggingSteps: "Étapes de journalisation",
        loggingStepsHelp: "Journalise les métriques d'entraînement toutes les N étapes. Des valeurs plus basses donnent des mises à jour plus fréquentes mais peuvent ralentir l'entraînement.",
        saveStrategy: "Stratégie de sauvegarde",
        saveStrategyHelp: "Quand sauvegarder les points de contrôle du modèle pendant l'entraînement.",
        saveStrategies: {
          no: "Pas de points de contrôle",
          epoch: "Après chaque époque (recommandé)",
          steps: "Toutes les N étapes",
        },
      },
      defaults: {
        showDefaults: "Utilisation des valeurs par défaut",
        cudaDefault: "Par défaut GPU : {{value}}",
        cpuDefault: "Par défaut CPU : {{value}}",
        resetToDefaults: "Réinitialiser aux valeurs par défaut",
        resetConfirm: "Toutes les valeurs de cette section seront réinitialisées à leurs valeurs par défaut.",
      },
      loraParams: {
        title: "Configuration LoRA",
        rank: "Rang (r)",
        rankHelp: "Dimension des matrices de faible rang. Des valeurs plus élevées capturent plus d'informations mais utilisent plus de mémoire et risquent le surapprentissage. Recommandé : 8-64.",
        alpha: "Alpha",
        alphaHelp: "Facteur d'échelle pour les poids LoRA. Généralement défini à 2x le rang. Des valeurs plus élevées augmentent l'impact du fine-tuning.",
        dropout: "Dropout",
        dropoutHelp: "Probabilité de dropout pour les couches LoRA. Aide à prévenir le surapprentissage. Recommandé : 0.05-0.1.",
        targetModules: "Modules cibles",
        targetModulesHelp: "Couches du modèle auxquelles appliquer LoRA. Plus de modules = plus de capacité de fine-tuning mais plus d'utilisation de mémoire.",
        modules: {
          q_proj: "Projection de requête (q_proj)",
          k_proj: "Projection de clé (k_proj)",
          v_proj: "Projection de valeur (v_proj)",
          o_proj: "Projection de sortie (o_proj)",
          gate_proj: "Projection de porte (gate_proj)",
          up_proj: "Projection montante (up_proj)",
          down_proj: "Projection descendante (down_proj)",
        },
        bias: "Entraînement du biais",
        biasHelp: "Comment gérer les termes de biais pendant l'entraînement. 'none' gèle les biais, 'lora_only' entraîne les biais LoRA, 'all' entraîne tous les biais.",
        biasOptions: {
          none: "Aucun (geler les biais)",
          lora_only: "LoRA uniquement",
          all: "Tous les biais",
        },
        useRslora: "Utiliser RSLoRA",
        useRsloraHelp: "Rank-Stabilized LoRA améliore la stabilité de l'entraînement et les performances pour les rangs élevés (r >= 64). Recommandé pour les grands rangs.",
        useDora: "Utiliser DoRA (Expérimental)",
        useDoraHelp: "Weight-Decomposed Low-Rank Adaptation peut améliorer la qualité du fine-tuning. Fonctionnalité expérimentale, peut augmenter le temps d'entraînement.",
        modulesToSave: "Modules à sauvegarder",
        modulesToSaveHelp: "Modules supplémentaires à entraîner entièrement (pas avec LoRA). Utile pour entraîner les couches de sortie comme lm_head.",
        saveModules: {
          lm_head: "Tête du modèle de langage (lm_head)",
          embed_tokens: "Embeddings de tokens (embed_tokens)",
        },
      },
      quantizationParams: {
        title: "Quantification",
        loadIn4bit: "Charger en 4 bits",
        loadIn4bitHelp: "Charge les poids du modèle en précision 4 bits pour réduire l'utilisation de la mémoire. Requis pour QLoRA avec une mémoire GPU limitée. Disponible uniquement sur les GPU CUDA.",
        quantType: "Type de quantification 4 bits",
        quantTypeHelp: "Algorithme de quantification 4 bits. NF4 (Normal Float 4) est recommandé pour une meilleure précision.",
        quantTypes: {
          nf4: "NF4 (recommandé)",
          fp4: "FP4",
        },
        doubleQuant: "Double quantification",
        doubleQuantHelp: "Applique une quantification secondaire pour réduire davantage la mémoire. Léger compromis de précision pour des économies de mémoire significatives.",
        computeDtype: "Type de données de calcul",
        computeDtypeHelp: "Type de données pour les calculs pendant l'entraînement. bfloat16 offre une meilleure stabilité numérique sur les GPU Ampere+ (RTX 3000+).",
        computeDtypes: {
          float16: "Float16 (recommandé)",
          bfloat16: "BFloat16 (RTX 3000+)",
          float32: "Float32 (précision complète)",
        },
        outputQuantization: "Quantification de sortie",
        outputQuantizationHelp: "Format de quantification pour le modèle GGUF final. q8_0 offre un bon équilibre entre taille et qualité.",
        outputTypes: {
          f32: "F32 (précision complète, plus grand)",
          f16: "F16 (demi-précision)",
          bf16: "BF16 (brain float 16)",
          q8_0: "Q8_0 (8 bits, recommandé)",
          auto: "Auto (laisser llama.cpp décider)",
        },
        cudaOnly: "Ces paramètres ne s'appliquent que lors de l'entraînement sur un GPU CUDA.",
      },
      modelfileParams: {
        title: "Modelfile Ollama",
        temperature: "Température",
        temperatureHelp: "Contrôle l'aléatoire de la sortie. Des valeurs plus basses rendent les réponses plus ciblées et déterministes, des valeurs plus élevées plus créatives. Recommandé : 0.7-0.9.",
        topP: "Top P (Échantillonnage par noyau)",
        topPHelp: "Ne considère que les tokens avec une probabilité cumulative jusqu'à cette valeur. Des valeurs plus basses se concentrent sur les tokens les plus probables. Recommandé : 0.9.",
        topK: "Top K",
        topKHelp: "Limite la sélection de tokens aux K options les plus probables. Des valeurs plus basses sont plus ciblées. Recommandé : 40.",
        system: "Prompt système",
        systemHelp: "Instructions qui définissent comment le modèle doit se comporter. Cela définit la personnalité et les capacités du modèle.",
        systemPlaceholder: "Vous êtes un assistant utile.",
        stop: "Séquences d'arrêt",
        stopHelp: "Séquences qui signalent au modèle d'arrêter la génération. Plusieurs séquences d'arrêt peuvent être ajoutées.",
        stopPlaceholder: "Entrez une séquence d'arrêt...",
        stopAdd: "Ajouter",
        repeatPenalty: "Pénalité de répétition",
        repeatPenaltyHelp: "Pénalité pour la répétition de tokens. Des valeurs plus élevées réduisent la répétition. Recommandé : 1.1.",
        repeatLastN: "Répéter les N derniers",
        repeatLastNHelp: "Nombre de tokens à vérifier pour la répétition. Des valeurs plus élevées considèrent plus de contexte. Recommandé : 64.",
        numCtx: "Taille de la fenêtre de contexte",
        numCtxHelp: "Longueur maximale du contexte pour l'inférence. Des fenêtres plus grandes permettent plus de contexte mais utilisent plus de mémoire.",
      },
    },
    dataFiles: {
      title: "Données d'entraînement",
      empty: "Pas encore de fichiers de données",
      dropzone: "Déposez des fichiers JSONL ici ou cliquez pour télécharger",
      dropzoneActive: "Déposez les fichiers ici...",
      uploadButton: "Télécharger un fichier",
      uploading: "Téléchargement...",
      deleteConfirm: "Supprimer ce fichier ?",
      previewRowCount: "{{count}} lignes au total",
      previewTruncated: "affichant les {{count}} premières",
      previewEmpty: "Ce fichier ne contient aucune ligne de données.",
      previewError: "Impossible de charger le contenu du fichier.",
      invalidRow: "Ligne invalide",
      showRawContent: "Afficher brut ({{size}})",
      rawContentTitle: "Contenu brut - Ligne {{line}}",
      rawContentLength: "{{count}} caractères",
      errorCount: "{{count}} ligne(s) invalide(s) trouvée(s)",
      fileStatus: {
        pending: "En attente de traitement",
        in_progress: "Chargement...",
        completed: "{{loaded}} lignes chargées ({{skipped}} ignorées)",
        failed: "Échec du chargement",
        skipped: "Ignoré",
      },
      validationErrors: {
        INVALID_JSON: "Syntaxe JSON invalide",
        NOT_OBJECT: "Doit être un objet JSON",
        MISSING_INSTRUCTION: "Champ \"instruction\" manquant",
        MISSING_OUTPUT: "Champ \"output\" manquant",
        INVALID_INSTRUCTION_TYPE: "\"instruction\" doit être une chaîne",
        INVALID_OUTPUT_TYPE: "\"output\" doit être une chaîne",
      },
    },
    training: {
      title: "Entraînement",
      startButton: "Créer le modèle",
      cancelButton: "Annuler l'entraînement",
      noModel: "Veuillez d'abord sélectionner un modèle",
      noDataFiles: "Veuillez ajouter au moins un fichier de données",
      readyDescription: "Tout est configuré. Cliquez sur le bouton ci-dessus pour commencer à créer votre modèle.",
      status: {
        idle: "Prêt",
        starting: "Démarrage...",
        loading_data: "Chargement des données...",
        loading_model: "Chargement du modèle...",
        training: "Entraînement...",
        exporting: "Exportation du modèle...",
        converting: "Conversion en GGUF...",
        completed: "Terminé",
        failed: "Échoué",
        cancelled: "Annulé",
      },
      progress: "Progression",
      step: "Étape {{current}} sur {{total}}",
      device: "Appareil",
      taskList: "Tâches",
      tasks: {
        detect_device: "Détecter le périphérique de calcul",
        import_libraries: "Importer les bibliothèques ML",
        load_model: "Charger le modèle de base",
        setup_lora: "Configurer l'adaptateur LoRA",
        tokenize: "Charger et tokeniser les données",
        train: "Entraîner le modèle",
        merge_lora: "Fusionner LoRA avec le modèle de base",
        convert_gguf: "Convertir au format GGUF",
        create_modelfile: "Créer le Modelfile Ollama",
        register_ollama: "Enregistrer le modèle dans Ollama",
      },
      taskWarnings: "{{count}} ligne(s) invalide(s) ignorée(s)",
      stillWorking: "Toujours en cours...",
      errorTitle: "Échec de l'entraînement",
      completed: "Entraînement terminé",
      completedDescription: "Votre modèle a été créé avec succès. Vérifiez le dossier de sortie pour le Modelfile.",
      cancelled: "L'entraînement a été annulé.",
    },
    ollama: {
      title: "Intégration Ollama",
      runButton: "Exécuter dans Ollama",
      modelName: "Nom du modèle",
      running: "Ouverture du terminal...",
    },
    presets: {
      title: "Préréglages d'entraînement",
      description: "Configurations de démarrage rapide optimisées pour différents cas d'utilisation. Sélectionnez un préréglage pour appliquer ses paramètres.",
      applyButton: "Appliquer",
      applyConfirmTitle: "Appliquer le préréglage ?",
      applyConfirmDescription: "Cela écrasera vos paramètres actuels d'entraînement, LoRA et quantification avec les valeurs du préréglage \"{{preset}}\". Cette action ne peut pas être annulée.",
      recommended: "Recommandé",
      allModels: "Tous les modèles",
      balanced: {
        name: "Équilibré",
        description: "Bon équilibre entre vitesse et qualité pour la plupart des tâches",
        pros: {
          versatile: "Fonctionne bien avec la plupart des modèles et données",
          stable: "Entraînement stable avec des valeurs par défaut éprouvées",
          good_defaults: "Bon point de départ pour l'expérimentation",
        },
        cons: {
          not_specialized: "Non optimisé pour des cas d'utilisation spécifiques",
          moderate_time: "Temps d'entraînement modéré",
        },
      },
      chat: {
        name: "Chat / Conversation",
        description: "Optimisé pour l'IA conversationnelle et le suivi d'instructions",
        pros: {
          natural_responses: "Réponses conversationnelles plus naturelles",
          instruction_following: "Meilleur suivi des instructions",
          diverse_outputs: "Sorties plus diverses et créatives",
        },
        cons: {
          more_memory: "Utilisation de mémoire plus élevée",
          longer_training: "Temps d'entraînement plus long",
        },
      },
      code: {
        name: "Génération de code",
        description: "Optimisé pour la programmation et la complétion de code",
        pros: {
          precise_syntax: "Apprentissage précis de la syntaxe",
          low_dropout: "Faible dropout pour la précision",
          all_modules: "Cible toutes les couches pertinentes",
        },
        cons: {
          more_memory: "Utilisation de mémoire plus élevée",
          slower_training: "Vitesse d'entraînement plus lente",
        },
      },
      fast: {
        name: "Itération rapide",
        description: "Entraînement rapide pour une expérimentation rapide",
        pros: {
          quick_results: "Résultats rapides pour les tests",
          low_memory: "Exigences mémoire réduites",
          rapid_testing: "Idéal pour le prototypage rapide",
        },
        cons: {
          lower_quality: "Qualité de sortie inférieure",
          less_learning: "Apprentissage moins approfondi",
        },
      },
      high_quality: {
        name: "Haute qualité",
        description: "Qualité maximale au prix du temps d'entraînement",
        pros: {
          best_results: "Meilleurs résultats possibles",
          thorough_learning: "Entraînement approfondi sur plus d'époques",
          all_modules: "Couverture complète des couches",
        },
        cons: {
          long_training: "Temps d'entraînement long",
          high_memory: "Exigences mémoire élevées",
          needs_gpu: "Nécessite un GPU puissant",
        },
      },
      low_memory: {
        name: "Faible mémoire",
        description: "Utilisation minimale de VRAM pour matériel limité",
        pros: {
          minimal_vram: "Utilisation minimale de VRAM",
          works_on_consumer: "Fonctionne sur les GPU grand public",
          gradient_accumulation: "Accumulation de gradient efficace",
        },
        cons: {
          slower_training: "Vitesse d'entraînement plus lente",
          smaller_rank: "Rang LoRA plus petit limite la capacité",
        },
      },
      multilingual: {
        name: "Multilingue",
        description: "Optimisé pour les modèles multilingues",
        pros: {
          language_diversity: "Préserve la diversité linguistique",
          balanced_learning: "Apprentissage équilibré entre les langues",
          longer_warmup: "Préchauffage étendu pour l'adaptation linguistique",
        },
        cons: {
          needs_diverse_data: "Nécessite des données d'entraînement diverses",
          moderate_time: "Temps d'entraînement modéré",
        },
      },
      reasoning: {
        name: "Raisonnement / Mathématiques",
        description: "Optimisé pour le raisonnement logique et les mathématiques",
        pros: {
          precise_learning: "Apprentissage précis et soigné",
          low_dropout: "Faible dropout pour la cohérence",
          consistent_outputs: "Sorties plus cohérentes",
        },
        cons: {
          more_epochs: "Plus d'époques d'entraînement nécessaires",
          higher_rank: "Rang plus élevé augmente la mémoire",
        },
      },
    },
    errors: {
      ERR_PROJECT_1001: "Un projet avec ce nom existe déjà.",
      ERR_PROJECT_1002: "Projet non trouvé.",
      ERR_PROJECT_1003: "Nom de projet invalide.",
      ERR_PROJECT_1004: "Le nom du projet ne peut pas être vide.",
      ERR_PROJECT_1005: "Échec de la création du projet.",
      ERR_PROJECT_1006: "Échec de la suppression du projet.",
      ERR_PROJECT_1007: "Échec de la mise à jour du projet.",
      ERR_PROJECT_1008: "Échec de l'ouverture du dossier du projet.",
      ERR_MODEL_2001: "Échec de la lecture de la configuration des modèles.",
      ERR_MODEL_2002: "Échec de l'écriture de la configuration des modèles.",
      ERR_DATA_3001: "Fichier de données non trouvé.",
      ERR_DATA_3002: "Type de fichier invalide. Seuls les fichiers JSONL sont autorisés.",
      ERR_DATA_3003: "Échec du téléchargement du fichier.",
      ERR_DATA_3004: "Échec de la suppression du fichier.",
      ERR_DATA_3005: "Échec de la lecture des fichiers de données.",
      ERR_TRAINING_4001: "Un travail d'entraînement est déjà en cours.",
      ERR_TRAINING_4002: "Aucun travail d'entraînement en cours.",
      ERR_TRAINING_4003: "Veuillez ajouter des fichiers de données avant de démarrer l'entraînement.",
      ERR_TRAINING_4004: "Un fichier de données d'entraînement n'a pas été trouvé.",
      ERR_TRAINING_4005: "Échec du chargement du modèle.",
      ERR_TRAINING_4006: "L'entraînement a échoué.",
      ERR_TRAINING_4007: "Échec de l'exportation du modèle.",
      ERR_TRAINING_4008: "L'entraînement a été annulé.",
      ERR_TRAINING_4009: "llama.cpp n'a pas été trouvé. Veuillez l'installer d'abord.",
      ERR_HF_5001: "Non connecté à Hugging Face.",
      ERR_HF_5002: "Échec de la connexion à Hugging Face.",
      ERR_HF_5003: "Token Hugging Face invalide.",
      ERR_OLLAMA_6001: "Ollama n'est pas installé.",
      ERR_OLLAMA_6002: "Ollama n'est pas en cours d'exécution.",
      ERR_OLLAMA_6003: "Échec de la création du modèle dans Ollama.",
      ERR_OLLAMA_6004: "Modèle non trouvé dans Ollama.",
      ERR_OLLAMA_6005: "Modelfile non trouvé. Veuillez d'abord entraîner le modèle.",
      ERR_OLLAMA_6006: "Échec de l'ouverture du terminal.",
      ERR_OLLAMA_6007: "Aucun modèle de base configuré. Veuillez d'abord sélectionner un modèle.",
      ERR_LLM_8001: "Format de clé API invalide.",
      ERR_LLM_8002: "Clé API rejetée. Veuillez vérifier votre clé.",
      ERR_LLM_8003: "Fournisseur inconnu.",
      ERR_LLM_8004: "Impossible d'atteindre l'API du fournisseur.",
      ERR_LLM_8005: "Impossible d'enregistrer la clé API.",
      ERR_DATA_SOURCE_9001: "Type de fichier invalide pour la source de données.",
      ERR_DATA_SOURCE_9002: "Le fichier est trop volumineux.",
      ERR_DATA_SOURCE_9003: "La source de données est vide.",
      ERR_GENERATION_9101: "Le fournisseur LLM n'est pas configuré.",
      ERR_GENERATION_9102: "Le modèle n'est pas disponible.",
      ERR_GENERATION_9103: "Limite de tokens dépassée. Veuillez utiliser des sources de données plus petites.",
      ERR_GENERATION_9104: "Erreur de communication avec l'API LLM.",
      ERR_GENERATION_9105: "Réponse invalide du LLM.",
      ERR_GENERATION_9106: "Limite de débit dépassée. Veuillez patienter et réessayer.",
      ERR_SAVE_9201: "Nom de fichier invalide.",
      ERR_SAVE_9202: "Échec de l'enregistrement du fichier.",
      unknown: "Une erreur inattendue s'est produite.",
    },
    validation: {
      mustBeInteger: "Doit être un nombre entier",
      mustBeNumber: "Doit être un nombre valide",
      mustBeString: "Doit être du texte",
      mustBeBoolean: "Doit être vrai ou faux",
      mustBeArray: "Doit être une liste",
      mustBeGreaterThan: "Doit être supérieur à {{min}}",
      mustBeAtLeast: "Doit être au moins {{min}}",
      mustBeAtMost: "Doit être au maximum {{max}}",
      maxLength: "Maximum {{maxLength}} caractères",
    },
  },
};

export default fr;
